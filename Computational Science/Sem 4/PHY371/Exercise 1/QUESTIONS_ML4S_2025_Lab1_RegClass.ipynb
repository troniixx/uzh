{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_1KwHwldVqS6"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Importing the Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "import seaborn as sns\n",
    "rcParams['figure.figsize'] = 10, 8\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYkhm8KPVqS_"
   },
   "source": [
    "### Lab 01: Exploring Regression and Classifications Tasks with Sklearn - <span style=\"color:#4ea373\"> **Graded** </span>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcvcvxe9VqTA"
   },
   "source": [
    "\n",
    "#### Instructions\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "* You can achieve up to **20 points** for this graded notebook. The points for each task are clearly declared in the task descriptions. Fill in the missing code fragments and answer questions whenever you see this symbol: &#x1F536;. Please do not change any of the provided code.\n",
    "\n",
    "* Team work is not allowed! Everybody implements his/her own code. Discussing issues with others is fine, sharing code with others is not.\n",
    "\n",
    "* If you use any code fragments found on the internet, make sure you reference them properly.\n",
    "\n",
    "* The responsible TA for this lab is Emilia, if you have further questions please reach out to her directly: emilia.arens@uzh.ch.\n",
    "\n",
    "* Since the lab sessions are specifically designed to answer your questions please make sure to attend those and only reach out if further questions pop up later.\n",
    "\n",
    "* Hand in your solution via OLAT until **27.03.2025**</span>. Make sure that all cells are execute as we will not rerun any code. Any cell that is not executed will automatically result in 0 points for this task.\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOtQTT4wVqTB",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Task Overview\n",
    "\n",
    "------------------------------------------------------------------------------------------\n",
    "**Regression**\n",
    "1. **Data Preparation** <span style=\"color:#4ea373\">**[4pt]**</span>\n",
    "\n",
    "    1.1. Load Data <span style=\"color:#4ea373\">[0pt]</span>\n",
    "\n",
    "    1.2. Clean Data <span style=\"color:#4ea373\">[1pt]</span>\n",
    "\n",
    "    1.3. Fill Missing Values <span style=\"color:#4ea373\">[3pt]</span>\n",
    "\n",
    "2. **Data Exploration** <span style=\"color:#4ea373\">**[2pt]**</span>\n",
    "\n",
    "    2.1. Correlation Heatmap <span style=\"color:#4ea373\">[1pt]</span>\n",
    "\n",
    "    2.2. Exploring Linear Relations <span style=\"color:#4ea373\">[1pt]</span>\n",
    "\n",
    "3. **Train Regression Models** <span style=\"color:#4ea373\">**[7.5pt]**</span>\n",
    "\n",
    "    3.1. Prepare Data for Training  <span style=\"color:#4ea373\">[1pt]</span>\n",
    "\n",
    "    3.2. Fit Regression Models  <span style=\"color:#4ea373\">[2pt]</span>\n",
    "\n",
    "    3.3. Feature Selection   <span style=\"color:#4ea373\">[1.5pt]</span>\n",
    "\n",
    "    3.4. Hyperparamter Tuning  <span style=\"color:#4ea373\">[1pt]</span>\n",
    "\n",
    "    3.5. Discretized Error Assessment  <span style=\"color:#4ea373\">[2pt]</span>\n",
    "\n",
    "-----------------------------------------------------------------------------------------\n",
    "**Classification**\n",
    "1. **Data Preparation**  <span style=\"color:#4ea373\">**[0pt]**</span>\n",
    "\n",
    "2. **Data Exploration**  <span style=\"color:#4ea373\">**[1.5pt]**</span>\n",
    "\n",
    "    2.1. Visualize the Target Label Distribution  <span style=\"color:#4ea373\">[0.5pt]</span>\n",
    "\n",
    "    2.2. Visualize Variance per Target Group  <span style=\"color:#4ea373\">[1pt]</span>\n",
    "\n",
    "3. **Train Classification Models**  <span style=\"color:#4ea373\">**[2pt]**</span>\n",
    "\n",
    "4. **Dealing with Data Imbalance**  <span style=\"color:#4ea373\">**[3pt]**</span>\n",
    "\n",
    "    4.1. Undersample Data  <span style=\"color:#4ea373\">[1pt]</span>\n",
    "\n",
    "    4.2. Oversample Data  <span style=\"color:#4ea373\">[2pt]</span>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgXYaVKDVqTB"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6iXpLSiVqTC"
   },
   "source": [
    "#### Regression\n",
    "\n",
    "**Task Description**\n",
    "\n",
    "Humans are very sensitive to humidity, as the skin relies on the air to get rid of moisture. The process of sweating is your body's attempt to keep cool and maintain its current temperature. If the air is at 100% relative humidity, sweat will not evaporate into the air. The humidity of the air, if it is not maintained at optimal levels, can be a factor that has adverse affects on people's health. According to reports, the human body is said to be most comfortable when the relative humidity of the area ranges between 20% and 60%.\n",
    "\n",
    "We investigate the change of the Relative Humidity (**RH**) by finding the correlation between the RH values and the all other attributes.\n",
    "\n",
    "**Dataset**\n",
    "\n",
    "The dataset contains 9358 instances of hourly averaged responses from an array of 5 metal oxide chemical sensors embedded in an Air Quality Chemical Multisensor Device. The device was located on the field in a significantly polluted area, at road level, within an Italian city. Data were recorded from March 2004 to February 2005 (one year) representing the longest freely available recordings of on field deployed air quality chemical sensor devices responses [1].\n",
    "\n",
    "<center>\n",
    "\n",
    "|Sl No| Attribute| Description|\n",
    "|-| -| -|\n",
    "|0| Date| Date (DD/MM/YYYY)|\n",
    "|1| Time| Time (HH.MM.SS)|\n",
    "|2| CO(GT)| True hourly averaged concentration CO in mg/m^3 (reference analyzer)|\n",
    "|3| PT08.S1(CO)| PT08.S1 (tin oxide) hourly averaged sensor response (nominally CO targeted)|\n",
    "|4| NMHC(GT) |True hourly averaged overall Non Metanic HydroCarbons concentration in microg/m^3 (reference analyzer)|\n",
    "|5| C6H6(GT)| True hourly averaged Benzene concentration in microg/m^3 (reference analyzer)|\n",
    "|6| PT08.S2(NMHC)| PT08.S2 (titania) hourly averaged sensor response (nominally NMHC targeted)|\n",
    "|7| NOx(GT)| True hourly averaged NOx concentration in ppb (reference analyzer)|\n",
    "|8| PT08.S3(NOx)| PT08.S3 (tungsten oxide) hourly averaged sensor response (nominally NOx targeted)|\n",
    "|9| NO2(GT)| True hourly averaged NO2 concentration in microg/m^3 (reference analyzer)|\n",
    "|10| PT08.S4(NO2)| PT08.S4 (tungsten oxide) hourly averaged sensor response (nominally NO2 targeted)|\n",
    "|11| PT08.S5(O3)| PT08.S5 (indium oxide) hourly averaged sensor response (nominally O3 targeted)|\n",
    "|12| T| Temperature in C|\n",
    "|13| RH| Relative Humidity (%)|\n",
    "|14| AH| AH Absolute Humidity|\n",
    "\n",
    "</center>\n",
    "\n",
    "\n",
    "**References:**\n",
    "\n",
    "[1] S. De Vito, E. Massera, M. Piga, L. Martinotto, G. Di Francia, On field calibration of an electronic nose for benzene estimation in an urban pollution monitoring scenario, Sensors and Actuators B: Chemical, Volume 129, Issue 2, 22 February 2008, Pages 750-757, ISSN 0925-4005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7szh7PVkVqTC"
   },
   "source": [
    "#### 1. Prepare the Dataset <span style=\"color:#4ea373\">[0pt]</span>\n",
    "\n",
    "\n",
    "\n",
    "Before getting started with model fitting, it is always a good idea to explore the data. What data types can be found in the dataframe, what is our target variable, are there missing values or messy data formats? We will therefore start with an initial data exploration.\n",
    "\n",
    "1.1. Load the dataset and display the first 5 rows. <span style=\"color:#4ea373\">[0pt]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "E4UO6YU8VqTD"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>TIME</th>\n",
       "      <th>CO_GT</th>\n",
       "      <th>PT08_S1_CO</th>\n",
       "      <th>NMHC_GT</th>\n",
       "      <th>C6H6_GT</th>\n",
       "      <th>PT08_S2_NMHC</th>\n",
       "      <th>NOX_GT</th>\n",
       "      <th>PT08_S3_NOX</th>\n",
       "      <th>NO2_GT</th>\n",
       "      <th>PT08_S4_NO2</th>\n",
       "      <th>PT08_S5_O3</th>\n",
       "      <th>T</th>\n",
       "      <th>RH</th>\n",
       "      <th>AH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004-03-10</td>\n",
       "      <td>18:00:00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>11.9</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1056.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>1692.0</td>\n",
       "      <td>1268.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>48.9</td>\n",
       "      <td>0.7578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-03-10</td>\n",
       "      <td>19:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1292.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>955.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1174.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1559.0</td>\n",
       "      <td>972.0</td>\n",
       "      <td>13.3</td>\n",
       "      <td>47.7</td>\n",
       "      <td>0.7255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-03-10</td>\n",
       "      <td>20:00:00</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1402.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>939.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>1555.0</td>\n",
       "      <td>1074.0</td>\n",
       "      <td>11.9</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.7502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-03-10</td>\n",
       "      <td>21:00:00</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1376.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>948.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>1092.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1584.0</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.7867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-03-10</td>\n",
       "      <td>22:00:00</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1272.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>836.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>1205.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>1490.0</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>59.6</td>\n",
       "      <td>0.7888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        DATE      TIME  CO_GT  PT08_S1_CO  NMHC_GT  C6H6_GT  PT08_S2_NMHC  \\\n",
       "0 2004-03-10  18:00:00    2.6      1360.0    150.0     11.9        1046.0   \n",
       "1 2004-03-10  19:00:00    2.0      1292.0    112.0      9.4         955.0   \n",
       "2 2004-03-10  20:00:00    2.2      1402.0     88.0      9.0         939.0   \n",
       "3 2004-03-10  21:00:00    2.2      1376.0     80.0      9.2         948.0   \n",
       "4 2004-03-10  22:00:00    1.6      1272.0     51.0      6.5         836.0   \n",
       "\n",
       "   NOX_GT  PT08_S3_NOX  NO2_GT  PT08_S4_NO2  PT08_S5_O3     T    RH      AH  \n",
       "0   166.0       1056.0   113.0       1692.0      1268.0  13.6  48.9  0.7578  \n",
       "1   103.0       1174.0    92.0       1559.0       972.0  13.3  47.7  0.7255  \n",
       "2   131.0       1140.0   114.0       1555.0      1074.0  11.9  54.0  0.7502  \n",
       "3   172.0       1092.0   122.0       1584.0      1203.0  11.0  60.0  0.7867  \n",
       "4   131.0       1205.0   116.0       1490.0      1110.0  11.2  59.6  0.7888  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining header\n",
    "col=['DATE','TIME','CO_GT','PT08_S1_CO','NMHC_GT','C6H6_GT','PT08_S2_NMHC',\n",
    "    'NOX_GT','PT08_S3_NOX','NO2_GT','PT08_S4_NO2','PT08_S5_O3','T','RH','AH']\n",
    "\n",
    "# Defining number of columns from csv\n",
    "use = list(np.arange(len(col)))\n",
    "\n",
    "# Reading the data from csv\n",
    "df_air = pd.read_csv(\n",
    "    '/Users/merterol/Desktop/iMac27_github/uzh/Computational Science/Sem 4/PHY371/Exercise 1/data/AirQualityUCI.csv',  # 🔶 Insert the correct path to the dataset\n",
    "    header=None, skiprows=1, names=col, na_filter=True, na_values=-200, usecols=use\n",
    ")\n",
    "\n",
    "# Format date column (See: https://docs.python.org/3/library/datetime.html) always a good idea for dates\n",
    "df_air['DATE'] = pd.to_datetime(df_air.DATE, format='%m/%d/%Y')\n",
    "\n",
    "# 🔶 Display the first few rows of the dataframe\n",
    "df_air.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbf75q3NVqTD"
   },
   "source": [
    "1.2. Clean the dataset <span style=\"color:#4ea373\">[1pt]</span>\n",
    "\n",
    "Start out by applying some simple cleaning steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_ZQDFPJRVqTE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataframe afer removing rows with NaN only: (9357, 15)\n",
      "\n",
      "Shape of the dataframe afer removing rows with 10 or more NaN values: (8991, 15)\n",
      "\n",
      "Remaining features with missing values:\n",
      "         Feature  Missing Values\n",
      "CO_GT      CO_GT            1647\n",
      "NMHC_GT  NMHC_GT            8104\n",
      "NOX_GT    NOX_GT            1595\n",
      "NO2_GT    NO2_GT            1598\n"
     ]
    }
   ],
   "source": [
    "# 🔶 Drop all rows ONLY containing NaN (not a number) for ALL features\n",
    "df_air = df_air.dropna(how='all')\n",
    "\n",
    "print(f\"Shape of the dataframe afer removing rows with NaN only: {df_air.shape}\\n\")\n",
    "\n",
    "# 🔶 Drop ONLY with 10 or more NaN values\n",
    "df_air = df_air.dropna(thresh=10)\n",
    "\n",
    "print(f\"Shape of the dataframe afer removing rows with 10 or more NaN values: {df_air.shape}\\n\")\n",
    "\n",
    "# 🔶 Check which features still contain missing values and how many values are missing\n",
    "incomplete_features = df_air.columns[df_air.isnull().any()]\n",
    "missing_values = df_air[incomplete_features].isnull().sum()\n",
    "\n",
    "print(\"Remaining features with missing values:\")\n",
    "print(pd.DataFrame({'Feature': incomplete_features, 'Missing Values': missing_values}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-M-M0glVqTE"
   },
   "source": [
    "1.3. Fill in the missing values <span style=\"color:#4ea373\">[3pt]</span>\n",
    "\n",
    "We want to keep as much data as possible, hence for the remaining NaN values we apply filling strategies rather than excluding incomplete samples.\n",
    "\n",
    "Fill the missing values with the following strategies:\n",
    "\n",
    "- CO_GT, NOX_GT, NO2_GT will be filled by monthly average of that particular hour\n",
    "- NHHC_GT will be dropped as it has 90% missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Higr3_urVqTF"
   },
   "outputs": [],
   "source": [
    "# 🔶  Split hour from 'TIME' into new column named 'HOUR'\n",
    "df_air['HOUR'] = ...\n",
    "\n",
    "# 🔶 Add the 'MONTH' column to the dataset you can get the month from the 'DATE' column\n",
    "df_air['MONTH'] = ...\n",
    "\n",
    "#drop the 'DATE' and 'TIME' columns\n",
    "df_air.drop(columns=['DATE', 'TIME'], inplace=True)\n",
    "\n",
    "# 🔶 Drop column NMHC_GT; it has 90% missing data\n",
    "df_air = ...\n",
    "\n",
    "# 🔶 Fill NaN values with monthly average of particular hour\n",
    "df_air = ...\n",
    "\n",
    "# 🔶 Check which features still contain missing values and how many values are missing\n",
    "incomplete_features = ...\n",
    "missing_values = ...\n",
    "\n",
    "print(\"Remaining features with missing values after filling with monthly mean of particular hour:\")\n",
    "print(pd.DataFrame({'Feature': incomplete_features, 'Missing Values': missing_values}))\n",
    "print()\n",
    "\n",
    "# 🔶 Fill the remaining NaN values with hourly average value\n",
    "df_air = ...\n",
    "\n",
    "# 🔶 Check which features still contain missing values and how many values are missing\n",
    "\n",
    "incomplete_features = ...\n",
    "missing_values = ...\n",
    "\n",
    "print(\"Remaining features with missing values after filling with hourly mean of particular hour:\")\n",
    "if len(incomplete_features) == 0:\n",
    "    print(\"No missing values\")\n",
    "else:\n",
    "    print(pd.DataFrame({'Feature': incomplete_features, 'Missing Values': missing_values}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vIIyisLVqTF"
   },
   "source": [
    "#### 2. Data Exploration <span style=\"color:#4ea373\">[2pt]</span>\n",
    "\n",
    "We now want to get an incentive on the relation between the variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9usA943wVqTF"
   },
   "source": [
    "2.1 Understand the Correlation between Variables <span style=\"color:#4ea373\">[1pt]</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zOCD4AbrVqTG"
   },
   "outputs": [],
   "source": [
    "# 🔶 Use a heatmap to explore the correlation between variables only numeric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWMY1DaWVqTG"
   },
   "source": [
    "Which conclusions can be drawn from the output? Comment on\n",
    "\n",
    "- value range\n",
    "- values on the diagonal\n",
    "- meaning of the values\n",
    "- expectations for modeling the data drawn from the correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6K_04w3hVqTG"
   },
   "source": [
    "&#x1F536; **Your Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWdcTFS0VqTG"
   },
   "source": [
    "2.2 Understand the Degree of Linearity between the Input Features and the Target <span style=\"color:#4ea373\">[1pt]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9OKBXHCjVqTG"
   },
   "outputs": [],
   "source": [
    "# 🔶 Plot all features (x-axis) against output variable RH (y-axis) in a 2 x 7 grid with `sns.regplot`. (Use a for loop and don't add the plots manually).\n",
    "# 🔶 Make sure that the regression curve is clearly visible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iVytXjpVqTH"
   },
   "source": [
    "Comment on the results. How does the heatplot relate to them?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrO0sL8NVqTH"
   },
   "source": [
    "&#x1F536; **Your Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7-8Htb2VqTH"
   },
   "source": [
    "#### 3. Training Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVSIwpdfVqTH"
   },
   "source": [
    "3.1. Data Preparations for Training  <span style=\"color:#4ea373\">[1pt]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4_6UWUqVqTH"
   },
   "outputs": [],
   "source": [
    "# 🔶 Define your train and test sets of variables. If you are unsure about the target variable, check the dataset description.\n",
    "X = ...\n",
    "y = ...\n",
    "\n",
    "print('Shape of X: {}'.format(X.shape))\n",
    "print('Shape of y: {}'.format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XnV4IGiVqTI"
   },
   "outputs": [],
   "source": [
    "# 🔶 Plot the distribution of the target variable as a histogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOuZ0-zIVqTI"
   },
   "outputs": [],
   "source": [
    "# 🔶 Split the data into training and testing sets using `train_test_split` with a test size of 0.3. Use the random seed 42.\n",
    "\n",
    "X_train, X_test, y_train, y_test = ...\n",
    "print('Number of train samples: {}'.format(X_train.shape))\n",
    "print('Number of test samples: {}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pb01ulDlVqTI"
   },
   "outputs": [],
   "source": [
    "# 🔶 Normalize the data using `StandardScaler`. Make sure to use the correct data subsets for the operation and don't forget to use the scaled data for all models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGN8R77ZVqTJ"
   },
   "source": [
    "3.2 Fit Regression Models  <span style=\"color:#4ea373\">[2pt]</span>\n",
    "\n",
    "We want to train different models in order to find the best performing one. Fit the following models with their standard hyperparamters. Make sure to set the random seed.\n",
    "- Linear Regression\n",
    "- Decision Tree Regression\n",
    "- Random Forest Regression\n",
    "- Support Vector Machine\n",
    "\n",
    "Then, predict the RH for the test set and report the RMSE for each of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JdmKcucVqTJ"
   },
   "outputs": [],
   "source": [
    "models = ['Linear Regression', 'Decision Tree', 'Random Forest', 'Support Vector Regression']\n",
    "\n",
    "def train_pipeline(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    rmse = {}\n",
    "\n",
    "    # 🔶 Train a linear regression model\n",
    "    lr = ...\n",
    "    rmse['Linear Regression'] = ...\n",
    "\n",
    "    # 🔶 Train a decision tree regression model\n",
    "    dt = ...\n",
    "    rmse['Decision Tree'] =...\n",
    "\n",
    "    # 🔶 Train a random forest regression model\n",
    "    rf = ...\n",
    "    rmse['Random Forest'] = ...\n",
    "\n",
    "    # 🔶 Train a support vector regression model\n",
    "    svr = ...\n",
    "    rmse['Support Vector Regression'] = ...\n",
    "    return rmse, [lr, dt, rf, svr]\n",
    "\n",
    "rmse, baseline_models = train_pipeline(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print('RMSE of all models:')\n",
    "print(pd.DataFrame(rmse.items(), columns=['Model', 'RMSE']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sx7Ptv03VqTJ"
   },
   "source": [
    "Comment on the results.\n",
    "- Which model would you pick.\n",
    "- Is the outcome surprising or did you assume a similar model ranking given your knowledge about the models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4o9wpTk4VqTJ"
   },
   "source": [
    "&#x1F536; **Your Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqk5y_vsVqTK"
   },
   "source": [
    "3.3. Feature Engineering  <span style=\"color:#4ea373\">[1.5pt]</span>\n",
    "\n",
    "The choice of input features can impact model performance drastically. Let's try to improve our baselines by choosing a different set of input features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9PQnCPEVqTK"
   },
   "outputs": [],
   "source": [
    "# 🔶 Write function to measure RMSE with different combinations of features.\n",
    "def train_test_RMSE(df_air, feat_, feat_name):\n",
    "    \"\"\"\n",
    "    The function train_test_RMSE returns the RMSE for different combinations\n",
    "    of features feat_ of the dataframe df_air.\n",
    "\n",
    "        :param df_air: (pandas.DataFrame) Our dataset\n",
    "        :param feat_: (List[str]) A list of column names\n",
    "        :param feat_name: (str) A string with the name of the feature combination\n",
    "        :return: (float) The RMSE score value\n",
    "    \"\"\"\n",
    "    X_ = ...\n",
    "    y_ = ...\n",
    "    X_train_, X_test_, y_train_, y_test_ = ...\n",
    "\n",
    "\n",
    "    rmse, _ = train_pipeline(X_train_, y_train_, X_test_, y_test_)\n",
    "    rmse = pd.DataFrame(rmse.items(), columns=['Model', 'RMSE'])\n",
    "    rmse['Features'] = feat_name\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ciiqi7rfVqTK"
   },
   "outputs": [],
   "source": [
    "# 🔶 Train the models with 3 different combinations of inputs.\n",
    "\n",
    "baseline = X.keys()[:]\n",
    "feature_list1 = ...\n",
    "feature_list2 = ...\n",
    "feature_list3 = ...\n",
    "\n",
    "feature_lists = [baseline, feature_list1, feature_list2, feature_list3]\n",
    "feature_names = ['Baseline', 'Feature List 1', 'Feature List 2', 'Feature List 3']\n",
    "\n",
    "rmse = pd.concat([train_test_RMSE(df_air, feat_, name) for feat_, name in zip(feature_lists, feature_names)])\n",
    "\n",
    "# 🔶 Provide a performance overview of our baseline and the three different feature combinations for all four models. Make a barplot with x = Model, y = RMSE, hue = Features\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gf4kN3-UVqTL"
   },
   "source": [
    "Comment on the results\n",
    "\n",
    "- Is the optimal set of features the same for all models?\n",
    "- Is there a relationship between the score and the heatmap in the previous lab? If so, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Dy6BRo7VqTL"
   },
   "source": [
    "&#x1F536; **Your Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fSIEDtbVqTL"
   },
   "source": [
    "3.4. Hyperparameter Tuning.  <span style=\"color:#4ea373\">[1pt]</span>\n",
    "\n",
    "The performance of a model does not only depend on the set of input features but also on the choice of hyperparameters that define the model. Ensemble models like random forests have especially many hyperparameters do choose. Let's make an informed choice by exploring different combinations of hyperparameters and evaluating their performance with `GridSearchCV`.\n",
    "\n",
    "- use different numbers of estimators\n",
    "- use cv of 5 or 10\n",
    "- use the correct scoring function\n",
    "- then, use the best model hyperparameters to predict on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mNiG6OpVqTL"
   },
   "outputs": [],
   "source": [
    "# 🔶 Your answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-NzJGjgVqTL"
   },
   "source": [
    "What are your conclusions regarding the Grid Search method. Did the performance improve? If not, what could be the reason?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usa0eqAaVqTQ"
   },
   "source": [
    "&#x1F536; **Your Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kaZt1D7NVqTQ"
   },
   "source": [
    "3.5. Discretized Error Assessment <span style=\"color:#4ea373\">[2pt]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOsDPi-TVqTQ"
   },
   "source": [
    "Let's investigate the performance of the random forest further. Use your random forest model obtained from the grid search.\n",
    "\n",
    "- Plot the box plots of **absolute** errors vs output range.\n",
    "- Discretize the output variable RH into three intervals of your choice.\n",
    "- Calculate the errors for each sample in the intervals.\n",
    "- Use `sns.boxplot` to plot the errors per interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NmjGQgm7VqTR"
   },
   "outputs": [],
   "source": [
    "# 🔶 Your answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8q4pBv6tVqTR"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bx_LkGH3VqTR"
   },
   "source": [
    "#### Classification\n",
    "\n",
    "**Task Description**\n",
    "\n",
    "We will work with a new dataset for solving a classification task. The new task is to classify the forest cover type given a set of variables describing the environmental surrounding of a location.\n",
    "\n",
    "## Dataset: Forest cover data\n",
    "This dataset contains 581012 tree observations from four areas of the Roosevelt National Forest in Colorado. All observations are cartographic variables (no remote sensing) from 30 meter x 30 meter sections of forest.\n",
    "\n",
    "This dataset includes information on tree type, shadow coverage, distance to nearby landmarks (roads etcetera), soil type, and local topography.\n",
    "\n",
    "<center>\n",
    "\n",
    "|Sl No| Attribute| Description|\n",
    "|-| -| -|\n",
    "|0| Elevation| Elevation in meters|\n",
    "|1| Aspect| Aspect in degrees azimuth|\n",
    "|2| Slope| Slope in degrees|\n",
    "|3| Horizontal_Distance_To_Hydrology|Horizontal distance to nearest surface water features|\n",
    "|4| Vertical_Distance_To_Hydrology|Vertical distance to nearest surface water features|\n",
    "|5| Horizontal_Distance_To_Roadways| Horizontal distance to nearest roadway|\n",
    "|6| Hillshade_9am| Hill shade index at 9am, summer solstice. Value out of 255|\n",
    "|7| Hillshade_Noon| Hill shade index at noon, summer solstice. Value out of 255|\n",
    "|8| Hillshade_3pm| Hill shade index at 3pm, summer solstice. Value out of 255|\n",
    "|9| Horizontal_Distance_To_Fire_Points| Horizontal distance to nearest wildfire ignition points|\n",
    "|10| Wilderness_Area1| Rawah Wilderness Area|\n",
    "|11| Wilderness_Area2| Neota Wilderness Area|\n",
    "|12| Wilderness_Area3| Comanche Peak Wilderness Area|\n",
    "|13| Wilderness_Area4| Cache la Poudre Wilderness Area|\n",
    "|14| Soil_Type1 ... Soil_Type40| 40 different Soil Types|\n",
    "|15| Cover Type| Forest Cover Type designation. Integer value between 1 and 7|\n",
    "\n",
    "</center>\n",
    "\n",
    "**Cover Types:**\n",
    "\n",
    "1. Spruce/Fir\n",
    "2. Lodgepole Pine\n",
    "3. Ponderosa Pine\n",
    "4. Cottonwood/Willow\n",
    "5. Aspen\n",
    "6. Douglas-fir\n",
    "7. Krummholz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MRg3XCnVqTR"
   },
   "source": [
    "#### 1. Prepare the Dataset <span style=\"color:#4ea373\">[0t]</span>\n",
    "\n",
    "Repeat the data preparation steps from before:\n",
    "- read the dataset file\n",
    "- look at the first 5 rows to check if the reading was successful\n",
    "- check if there are NaN values and drop the respective rows if needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sjw-uWCGVqTR"
   },
   "outputs": [],
   "source": [
    "# 🔶 Read the dataset and show the first 5 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "do83CksqVqTR"
   },
   "outputs": [],
   "source": [
    "# 🔶 Check if there are any missing values\n",
    "\n",
    "\n",
    "# 🔶 Remove rows with NaN values if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhig36PFVqTR"
   },
   "source": [
    "#### 2. Explore the Dataset <span style=\"color:#4ea373\">[1.5pt]</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldyUVSWfVqTR"
   },
   "source": [
    "2.1. Visualize the target label distribution  <span style=\"color:#4ea373\">[0.5pt]</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Xz02g_DVqTS"
   },
   "outputs": [],
   "source": [
    "# 🔶 plot the number of samples per covertype by using sns.countplot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDqontkDVqTS"
   },
   "source": [
    "2.2 Explore the data distribution per feature per target class.  <span style=\"color:#4ea373\">[1pt]</span>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJ42p1-SVqTS"
   },
   "outputs": [],
   "source": [
    "# 🔶 make a boxplot for the first 12 features where you plot the cover type against the feature values.\n",
    "# 🔶 Use a loop and arrange the plots in a 5 x 2 grid\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpwSYRmZVqTS"
   },
   "source": [
    "Comment on the results.\n",
    "\n",
    "- which features do you expect to be important for high classification accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjRb_yA0VqTS"
   },
   "source": [
    "&#x1F536; **Your Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIPElxbYVqTS"
   },
   "source": [
    "#### 3. Model Training <span style=\"color:#4ea373\">[2pt]</span>\n",
    "\n",
    "Train the classification models on the first 10 features\n",
    "\n",
    "- Make sure to prepare the data correctly\n",
    "- Train a Logistic Regression Model\n",
    "- Train a Decision Tree Model\n",
    "- Train a Random Forest Classifier\n",
    "- Train a K Nearest Neighbor Model\n",
    "\n",
    "Report the Accuracy on the test set for each of the models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-S6qb8ROVqTT"
   },
   "outputs": [],
   "source": [
    "# 🔶 Your answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TueW3bnVqTT"
   },
   "source": [
    "Comment on the results.\n",
    "- Which model would you pick.\n",
    "- Is the outcome surprising or did you assume a similar model ranking given your knowledge about the models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSCPXThDVqTT"
   },
   "source": [
    "&#x1F536; **Your Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fd7MsV7DVqTT"
   },
   "source": [
    "#### 4. Dealing with imbalanced data<span style=\"color:#4ea373\">[3pt]</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHAh-BvUVqTT"
   },
   "source": [
    "An imbalanced classification problem is an example of a classification problem where the distribution of examples across the known classes is biased or skewed. The distribution can vary from a slight bias to a severe imbalance where there is one example in the minority class against hundreds, thousands, or millions of examples in the majority class or classes.\n",
    "\n",
    "Imbalanced classifications pose a challenge for predictive modeling as most of the machine learning algorithms used for classification were designed around the assumption of an equal number of examples for each class. This results in models that have poor predictive performance, specifically for the minority class. This is a problem because typically, the minority class is more important and therefore the problem is more sensitive to classification errors for the minority class than the majority class.\n",
    "\n",
    "A commonly used strategy for counterbalancing the dataset is resampling. There are two main methods that you can use to even-up the classes:\n",
    "\n",
    "1. You can add copies of instances from the under-represented class called over-sampling (or more formally sampling with replacement)\n",
    "2. You can delete instances from the over-represented class, called under-sampling.\n",
    "\n",
    "Let's retrain the Random Forest classifiers with both under-sampled data and over-sampled data, and compare their accuracy scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCneBWlcVqTU"
   },
   "source": [
    "4.1. Undersampling <span style=\"color:#4ea373\">[1pt]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Wo8eMx_VqTU"
   },
   "outputs": [],
   "source": [
    "# 🔶 print the size of the smallest class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kGfnGw0uVqTU"
   },
   "outputs": [],
   "source": [
    "# 🔶 Undersample all the majority classes so that all classes have the same cardinality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEcgavzLVqTU"
   },
   "outputs": [],
   "source": [
    "# 🔶 plot the number of samples per covertype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-BldXm1mVqTU"
   },
   "outputs": [],
   "source": [
    "# 🔶 Fit a new Random Forest Classifier for this data subset and print it's performance.\n",
    "# 🔶 (Hint: Don't forget to prepare the data correctly)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxhdUzoeVqTU"
   },
   "source": [
    "4.2 Oversampling <span style=\"color:#4ea373\">[2pt]</span>\n",
    "\n",
    "Repeat the procedure but this time oversample the underrepresented classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JtB9VwJdVqTU"
   },
   "outputs": [],
   "source": [
    "# 🔶 print the size of the largest class\n",
    "\n",
    "# 🔶 Oversample all the majority classes so that all classes have the same cardinality.\n",
    "\n",
    "\n",
    "# 🔶 plot the number of samples per covertype\n",
    "\n",
    "\n",
    "# 🔶 Fit a new Random Forest Classifier for this data subset and print it's performance.\n",
    "# 🔶 (Hint: Don't forget to prepare the data correctly)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8q39QTBGVqTV"
   },
   "source": [
    "Comment on the results. Does one method clearly outperform the other? If so, can you think of a reason for that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUc-63G0VqTV"
   },
   "source": [
    "&#x1F536; **Your Answer**:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
