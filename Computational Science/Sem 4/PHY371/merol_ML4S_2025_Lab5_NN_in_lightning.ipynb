{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Lightning\n",
    "\n",
    "This lab is design for investigating the basic architecture and parameter searching techniques in deep learning literature. We use a fundamental dataset, MNIST, with the help of PyTorch Lightning and design our model to handle image classification task.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* You can achieve up to **20 points** for this graded notebook. The points for each task are clearly declared in the task descriptions. Fill in the missing code fragments and answer questions whenever you see this symbol: &#x1F536;. Please do not change any of the provided code. Notice that one symbol &#x1F536; does NOT mean one-line code: sometimes it can require several code of lines.\n",
    "\n",
    "* Team work is not allowed! Everybody implements his/her own code. Discussing issues with others is fine, sharing code with others is not. \n",
    "\n",
    "* If you use any code fragments found on the internet, make sure you reference them properly.\n",
    "\n",
    "* The responsible TA for this lab are **Yuchang** and **Sara**, if you have further questions please reach out to them directly: **yuchang.jiang@uzh.ch** and **sara.zoccheddu@uzh.ch**.\n",
    "\n",
    "* Since the lab sessions are specifically designed to answer your questions please make sure to attend those and only reach out if further questions pop up later.\n",
    "\n",
    "* Hand in your solution via OLAT until <span style=\"color:#4ea373\">**15.05.2025**</span>. Make sure that all cells are execute as we will not rerun any code. Any cell that is not executed will automatically result in 0 points for this task. \n",
    "</div>\n",
    "\n",
    "#### Suggestions\n",
    "\n",
    "Please install pytorch lightning packages via conda or pip before starting the lab session. You can use the tutorials of [Pytorch Lightning](https://lightning.ai/docs/pytorch/stable/notebooks/lightning_examples/mnist-hello-world.html).\n",
    "\n",
    "Please also check: [Tensorboard](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html)\n",
    "\n",
    "For installing lightning: [Link](https://lightning.ai/pytorch-lightning)\n",
    "\n",
    "Please keep in mind that this is a valuable opportunity to develop self-learning skills. When working with a new package like PyTorch Lightning, always refer to the official documentation or search for solutions online when encountering errors, rather than immediately asking a friend or TA. This habit will greatly enhance your ability to troubleshoot and learn independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task Overview\n",
    "\n",
    "------------------------------------------------------------------------------------------\n",
    "\n",
    "1. **Datasets** <span style=\"color:#4ea373\">**[3pt]**</span>\n",
    "2. **NN Architectures** <span style=\"color:#4ea373\">**[5pt]**</span>\n",
    "2. **Experiments** <span style=\"color:#4ea373\">**[5pt]**</span>\n",
    "2. **Results** <span style=\"color:#4ea373\">**[4pt]**</span>\n",
    "2. **Conclusions** <span style=\"color:#4ea373\">**[3pt]**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Basic Machine Learning Modules\n",
    "import pandas\n",
    "import numpy\n",
    "import sklearn\n",
    "\n",
    "# Deep Learning Modules\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "# Visualization Modules\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Others\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torchmetrics import Accuracy\n",
    "from lightning.pytorch.loggers import CSVLogger, TensorBoardLogger\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import TQDMProgressBar\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # You can comment out this line while debuging, not in submission!\n",
    "\n",
    "numpy.random.seed(42)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "torch.manual_seed(42)\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "DATASET_PATH = '/Users/merterol/Desktop/iMac27_github/uzh/Computational Science/Sem 4/PHY371/MNIST'  # You can change them\n",
    "EXPERIMENTS_PATH = '/Users/merterol/Desktop/iMac27_github/uzh/Computational Science/Sem 4/PHY371/MNIST_exp'  # You can change them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Dataset (3 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Design a DataModule for MNIST.\n",
    "- Use 80/20 % split for train/val sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 43 (2743504745.py, line 47)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 47\u001b[0;36m\u001b[0m\n\u001b[0;31m    def test_dataloader(self) -> torch.utils.data.DataLoader:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 43\n"
     ]
    }
   ],
   "source": [
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_folder: str = DATASET_PATH, batch_size: int = 64, num_cpu: int = 1):\n",
    "        super().__init__()\n",
    "        self.path = data_folder\n",
    "        self.batch_size = batch_size\n",
    "        self.num_cpu = num_cpu\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    \n",
    "    def prepare_data(self) -> None:\n",
    "        # ðŸ”¶ TODO: Download MNIST Data in self.path\n",
    "\n",
    "    \n",
    "    def setup(self, stage: str = 'fit') -> None:\n",
    "        # ðŸ”¶ TODO: Insert your code \n",
    "        # HINT: how to split the whole dataset into train, val sets? \n",
    "        # and how should those sets be used in different stages?\n",
    "        \n",
    "        if stage in ['fit', 'tune']:\n",
    "            self.train_dataset = ...  # ðŸ”¶ TODO: Insert your code \n",
    "        \n",
    "        if stage in ['fit', 'tune', 'validate']:\n",
    "            self.val_dataset = ...  # ðŸ”¶ TODO: Insert your code \n",
    "        \n",
    "        elif stage in ['test', 'predict']:\n",
    "            self.test_dataset = ...  # ðŸ”¶ TODO: Insert your code \n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError('Unknown Stage: {}'.format(stage))\n",
    "            \n",
    "    def train_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        return torch.utils.data.DataLoader(\n",
    "            batch_size=self.batch_size, num_workers=self.num_cpu,  # DO NOT CHANGE IN VAL/TEST LOADERS\n",
    "            dataset=self.train_dataset, shuffle=True,  # Could be changed in val/test loaders\n",
    "        )\n",
    "    def val_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        # ðŸ”¶ TODO: Insert your code\n",
    "        # HINT: check docs of 'torch.utils.data.DataLoader'.\n",
    "    \n",
    "    def test_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        # ðŸ”¶ TODO: Insert your code\n",
    "        # HINT: what's the difference between 'val_dataloader' and 'test_dataloader'?\n",
    "    \n",
    "    def predict_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        return self.test_dataloader()\n",
    "\n",
    "data_module = MNISTDataModule()\n",
    "data_module.prepare_data()\n",
    "data_module.setup('fit')\n",
    "dl = data_module.train_dataloader()\n",
    "print(next(dl.__iter__()))  # DO NOT CHANGE | will be used for checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do not change! Only for checking.\n",
    "print('Shape of Images: [B x C x H x W] = ', next(dl.__iter__())[0].shape)\n",
    "print('Shape of Labels: [B] = ', next(dl.__iter__())[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Neural Network Architecture (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Design an model with 3 Convolutional layers and 1 fully-connected layer, in this order:\n",
    "    - Convolution: Kernel size = 3x3, padding = 'same', number of filters = 4\n",
    "    - Convolution: Kernel size = 3x3, padding = 'same', number of filters = 8\n",
    "    - Convolution: Kernel size = 3x3, padding = 'same', number of filters = 4\n",
    "    - Linear: No Bias, num_class = 10 in MNIST Dataset\n",
    "- Use rectified linear unit (ReLU) for activation function (when necessary). \n",
    "- Initialize all the weights with `xavier_uniform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RawModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # ðŸ”¶ TODO: Insert your code\n",
    "        # HINT: define model layers based on the given model design (Conv2d...)\n",
    "        \n",
    "        self.model = ...\n",
    "        self.model.apply(self.initialize_weights)\n",
    "    \n",
    "    @staticmethod\n",
    "    def initialize_weights(module: torch.nn.Module) -> None:\n",
    "        # ðŸ”¶ TODO: Insert your code\n",
    "        \n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Input: torch.Tensor | dtype=torch.float | shape=[B, C, H, W]\n",
    "        Output: torch.Tensor | dtype=torch.float | shape=[B, num_class]\n",
    "        \"\"\"\n",
    "        # ðŸ”¶ TODO: Insert your code\n",
    "\n",
    "model = RawModel()\n",
    "with torch.no_grad():  # DO NOT REMOVE THIS LINES BELOW\n",
    "    sample_image = torch.rand(size=(4, 1, 28, 28))\n",
    "    output = model(sample_image)\n",
    "    print(output.shape, output.dtype)  \n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Experiments (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define training/validation/test step and optimizer with cross-entropy loss and `AdamW` optimizer.\n",
    "- Use accuracy scores for monitoring the experiment. (you can use multiclass accuracy from Lightning Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MNISTExperiment(pl.LightningModule):\n",
    "    def __init__(self, learning_rate: float = 1e-3):\n",
    "        super().__init__()\n",
    "        self.model = RawModel()\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.train_scores = ...  # ðŸ”¶ TODO: Insert your code\n",
    "        # HINT: define the metric used in this classification task.\n",
    "        self.validation_scores = ...  # ðŸ”¶ TODO: Insert your code\n",
    "        self.test_scores = ...  # ðŸ”¶ TODO: Insert your code\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        \n",
    "         # ðŸ”¶ TODO: Insert your code: Loss Calculation\n",
    "        loss = ...\n",
    "        # ðŸ”¶ TODO: Insert your code: Score Calculation\n",
    "        \n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_accuracy', ...)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx) -> None:\n",
    "        # ðŸ”¶ TODO: Insert your code\n",
    "        self.log('validation_accuracy', ...)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx) -> None:\n",
    "        # ðŸ”¶ TODO: Insert your code\n",
    "        self.log('test_accuracy', ...)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = ...  # ðŸ”¶ TODO: Insert your code \n",
    "        return optimizer\n",
    "\n",
    "experiment = MNISTExperiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define a lightning `Trainer`:\n",
    "    - Maximum epoch = 20\n",
    "    - accelerator = 'auto'\n",
    "    - Use `CSVLogger` and `TensorBoardLogger`\n",
    "    - Use `EarlyStopping` with patience epoch = 3\n",
    "    - Use `TQDMProgressBar` with refresh rate = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    ...  # ðŸ”¶ Insert your code\n",
    "    # HINT: check docs of pytorch lightning Trainer, and understand its flags.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Results (4 Points)\n",
    "- Train and test the `RawModel` and plot the score and loss values versus epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ðŸ”¶ Insert your code\n",
    "# HINT: trainer.fit(...) and trainer.test(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Re-design the model with `Dropout` layer in-between the convolutional layers and re-train the model. as like `RawModel` and create a new module as `ModelWithDropout`. Try:\n",
    "    - dropout probability = 0.1\n",
    "    - dropout probability = 0.5\n",
    "    - dropout probability = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ðŸ”¶ Insert your code\n",
    "# HINT: \n",
    "# re-create a new model class like 'class RawModel(torch.nn.Module)'. \n",
    "# add Dropout layers.\n",
    "# then run trainer.fit(...) and trainer.test(...) with this new model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Re-design the model with `BatchNorm` layer in-between the convolutional layers and re-train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ðŸ”¶ Insert your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Conclusion (3 Point)\n",
    "Comment on your findings:\n",
    "- Which method is better? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: ... # ðŸ”¶ TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Are the results statistically significant? If not, how can we get significant ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: ... # ðŸ”¶ TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Comment on different normalization techniques: `BatchNorm`, `LayerNorm` , `InstanceNorm` and `GroupNorm`. Explain the purpose of usage of them in general (for the RGB datasets, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: ... # ðŸ”¶ TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Explain the operations of the `Dropout` layer in training and testing phase. Is there any difference, or are they always the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: ... # ðŸ”¶ TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Explain the difference between `AdamW` and `Adam` optimizers in a few sentences.\n",
    "\n",
    "Hint: https://arxiv.org/abs/1711.05101 for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: ... # ðŸ”¶ TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compare the Kaiming (He) and Xavier (Glorot) Initialization techniques and explain their differences in a few sentences. \n",
    "\n",
    "Hint: You can find more information from their original papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: ... # ðŸ”¶ TODO"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
