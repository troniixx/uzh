{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 8: Text encoding, text formats, command-line interfaces\n",
    "\n",
    "**NOTE:** All of the content of this lecture is in the PDF slides. This notebook only contains the code snippets and exercises.\n",
    "\n",
    "Some of the code in this notebook requires example files, which can be downloaded from OLAT."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's in a string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_string(string, encoding):\n",
    "    chars_encoded = [(char, char.encode(encoding)) for char in string]\n",
    "    chars = []\n",
    "    bytes = []\n",
    "    for char, encoded in chars_encoded:\n",
    "        for byte in encoded:\n",
    "            chars.append(char)\n",
    "            char = None\n",
    "            bytes.append(byte)\n",
    "    blank = \" \" * 8\n",
    "    print(\"         Characters:\", \" \".join(f\"{char:<8}\" if char else blank for char in chars))\n",
    "    print(\"Unicode code points:\", \" \".join(f\"{ord(char):<8}\" if char else blank for char in chars))\n",
    "    print(\"Encoded bytes (bin):\", \" \".join(f\"{byte:08b}\" for byte in bytes))\n",
    "    print(\"Encoded bytes (hex):\", \" \".join(f\"{byte:02x}\".ljust(8) for byte in bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Characters: H        e        l        l        o       \n",
      "Unicode code points: 72       101      108      108      111     \n",
      "Encoded bytes (bin): 01001000 01100101 01101100 01101100 01101111\n",
      "Encoded bytes (hex): 48       65       6c       6c       6f      \n"
     ]
    }
   ],
   "source": [
    "analyze_string(\"Hello\", \"ASCII\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latin-1:\n",
      "         Characters: h        Ã¤       \n",
      "Unicode code points: 104      228     \n",
      "Encoded bytes (bin): 01101000 11100100\n",
      "Encoded bytes (hex): 68       e4      \n",
      "\n",
      "UTF-8:\n",
      "         Characters: h        Ã¤                \n",
      "Unicode code points: 104      228              \n",
      "Encoded bytes (bin): 01101000 11000011 10100100\n",
      "Encoded bytes (hex): 68       c3       a4      \n",
      "\n",
      "UTF-16:\n",
      "         Characters: h                 Ã¤                \n",
      "Unicode code points: 104               228              \n",
      "Encoded bytes (bin): 00000000 01101000 00000000 11100100\n",
      "Encoded bytes (hex): 00       68       00       e4      \n",
      "\n",
      "UTF-32:\n",
      "         Characters: h                                   Ã¤                                  \n",
      "Unicode code points: 104                                 228                                \n",
      "Encoded bytes (bin): 00000000 00000000 00000000 01101000 00000000 00000000 00000000 11100100\n",
      "Encoded bytes (hex): 00       00       00       68       00       00       00       e4      \n"
     ]
    }
   ],
   "source": [
    "print(\"Latin-1:\")\n",
    "analyze_string(\"hÃ¤\", \"Latin-1\")\n",
    "print()\n",
    "print(\"UTF-8:\")\n",
    "analyze_string(\"hÃ¤\", \"utf-8\")\n",
    "print()\n",
    "print(\"UTF-16:\")\n",
    "analyze_string(\"hÃ¤\", \"utf-16be\")\n",
    "print()\n",
    "print(\"UTF-32:\")\n",
    "analyze_string(\"hÃ¤\", \"utf-32be\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding/decoding strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hi! \\xf0\\x9f\\xa4\\x93'\n",
      "72\n",
      "<class 'bytes'>\n"
     ]
    }
   ],
   "source": [
    "my_string = \"Hi! ðŸ¤“\"\n",
    "my_bytes = my_string.encode(\"utf-8\")\n",
    "print(my_bytes)\n",
    "\n",
    "# codepoint for H\n",
    "print(my_bytes[0])\n",
    "\n",
    "print(type(my_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! ðŸ¤“\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "text = my_bytes.decode(\"utf-8\")\n",
    "print(text)\n",
    "\n",
    "print(type(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening and reading files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='/Users/merterol/uzh/Computational Linguistics/Sem 2/PCL 2/Lecture/Lecture 8/my_file.txt' mode='r' encoding='utf-8'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_file = open(\"/Users/merterol/uzh/Computational Linguistics/Sem 2/PCL 2/Lecture/Lecture 8/my_file.txt\")\n",
    "my_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi! ðŸ¤“\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ascii' codec can't decode byte 0xf0 in position 4: ordinal not in range(128)\n"
     ]
    }
   ],
   "source": [
    "# Wrong encoding!\n",
    "try:\n",
    "    my_file = open(\"/Users/merterol/uzh/Computational Linguistics/Sem 2/PCL 2/Lecture/Lecture 8/my_file.txt\", encoding=\"ascii\")\n",
    "    my_file.read()\n",
    "except UnicodeDecodeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi! Ã°\\x9fÂ¤\\x93\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wrong encoding!\n",
    "my_file = open(\"/Users/merterol/uzh/Computational Linguistics/Sem 2/PCL 2/Lecture/Lecture 8/my_file.txt\", encoding=\"latin-1\")\n",
    "my_file.read()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Byte mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedReader name='/Users/merterol/uzh/Computational Linguistics/Sem 2/PCL 2/Lecture/Lecture 8/my_file.txt'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_file = open(\"/Users/merterol/uzh/Computational Linguistics/Sem 2/PCL 2/Lecture/Lecture 8/my_file.txt\", \"rb\")\n",
    "my_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Hi! \\xf0\\x9f\\xa4\\x93\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_file.read()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unicode normal forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NFD: heÌteÌrogeÌneÌiteÌ 18\n",
      "NFC: hÃ©tÃ©rogÃ©nÃ©itÃ© 13\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "word = \"hÃ©tÃ©rogÃ©nÃ©itÃ©\"\n",
    "nfd_normalized = unicodedata.normalize(\"NFD\", word) # use combining chars\n",
    "print(\"NFD:\", nfd_normalized, len(nfd_normalized))\n",
    "nfc_normalized = unicodedata.normalize(\"NFC\", word) # use single chars\n",
    "print(\"NFC:\", nfc_normalized, len(nfc_normalized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fun with emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘¨â€ðŸ¦°\n",
      "ðŸ‘©â€ðŸ¦²\n",
      "ðŸ«±ðŸ¼â€ðŸ«²ðŸ½\n",
      "ðŸ‘©ðŸ¿â€â¤â€ðŸ’‹â€ðŸ‘¨ðŸ»\n"
     ]
    }
   ],
   "source": [
    "PERSON = \"\\U0001F9D1\"\n",
    "MAN = \"\\U0001F468\"\n",
    "WOMAN = \"\\U0001F469\"\n",
    "SKIN_TONE_1 = \"\\U0001F3FB\"\n",
    "SKIN_TONE_2 = \"\\U0001F3FC\"\n",
    "SKIN_TONE_3 = \"\\U0001F3FD\"\n",
    "SKIN_TONE_4 = \"\\U0001F3FE\"\n",
    "SKIN_TONE_5 = \"\\U0001F3FF\"\n",
    "ZERO_WIDTH_JOINER = \"\\U0000200D\"\n",
    "RIGHTWARDS_HAND = \"\\U0001FAF1\"\n",
    "LEFTWARDS_HAND = \"\\U0001FAF2\"\n",
    "HEART = \"\\U00002764\"\n",
    "KISS = \"\\U0001F48B\"\n",
    "RED_HAIR = \"\\U0001F9B0\"\n",
    "BLOND_HAIR = \"\\U0001F471\"\n",
    "BALD = \"\\U0001F9B2\"\n",
    "WHITE_HAIR = \"\\U0001F9B3\"\n",
    "\n",
    "print(MAN + ZERO_WIDTH_JOINER + RED_HAIR)\n",
    "print(WOMAN + ZERO_WIDTH_JOINER + BALD)\n",
    "print(RIGHTWARDS_HAND + SKIN_TONE_2 + ZERO_WIDTH_JOINER + LEFTWARDS_HAND + SKIN_TONE_3)\n",
    "print(WOMAN + SKIN_TONE_5 + ZERO_WIDTH_JOINER + HEART + ZERO_WIDTH_JOINER + KISS + ZERO_WIDTH_JOINER + MAN + SKIN_TONE_1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-based data formats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['date', 'author', 'text']\n",
      "['2018-05-01', 'Philomena Cunk', 'What a wonderful day']\n",
      "['', 'Guido van Rossum', 'Hello, world!']\n",
      "['2006-08-04', 'Borat Sagdiyev', 'This suit is black...\\n\\nNOT!']\n",
      "['1637-08-04', 'Pierre de Fermat', 'aâ¿ + bâ¿ = câ¿ for n > 2']\n"
     ]
    }
   ],
   "source": [
    "with open(\"/Users/merterol/uzh/Computational Linguistics/Sem 2/PCL 2/Lecture/Lecture 8/tweets.csv\", encoding=\"utf-8\") as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    for row in reader:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date': '2018-05-01', 'author': 'Philomena Cunk', 'text': 'What a wonderful day'}\n",
      "{'date': '', 'author': 'Guido van Rossum', 'text': 'Hello, world!'}\n",
      "{'date': '2006-08-04', 'author': 'Borat Sagdiyev', 'text': 'This suit is black...\\n\\nNOT!'}\n",
      "{'date': '1637-08-04', 'author': 'Pierre de Fermat', 'text': 'aâ¿ + bâ¿ = câ¿ for n > 2'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"/Users/merterol/uzh/Computational Linguistics/Sem 2/PCL 2/Lecture/Lecture 8/tweets.csv\", encoding=\"utf-8\") as infile:\n",
    "\treader = csv.DictReader(infile)\n",
    "\tfor row in reader:\n",
    "        \tprint(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"new.csv\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow([\"name\", \"age\"])\n",
    "    writer.writerow([\"Martha\", 36])\n",
    "    writer.writerow([\"Carl\", 19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"new.csv\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    writer = csv.DictWriter(outfile, [\"name\", \"age\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerow({\"name\": \"Martha\", \"age\": 36})\n",
    "    writer.writerow({\"name\": \"Carl\", \"age\": 19})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tweets': [{'date': {'year': 2018, 'month': 5, 'day': 1},\n",
       "   'author': 'Philomena Cunk',\n",
       "   'text': 'What a wonderful day'},\n",
       "  {'author': 'Borat Sagdiyev', 'text': 'This suit is black...\\n\\nNOT!'},\n",
       "  {'date': None,\n",
       "   'author': 'Pierre de Fermat',\n",
       "   'text': 'aâ¿ + bâ¿ = câ¿ for n > 2'}]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/Users/merterol/uzh/Computational Linguistics/Sem 2/PCL 2/Lecture/Lecture 8/tweets.json\", encoding=\"utf-8\") as infile:\n",
    "    data = json.load(infile)\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/merterol/uzh/Computational Linguistics/Sem 2/PCL 2/Lecture/Lecture 8/tweets.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    data = {\"example\": [1, 2, 3]}\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading XML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xml.etree.ElementTree.ElementTree at 0x10d2d48f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = ET.parse(\"/Users/merterol/uzh/Computational Linguistics/Sem 2/PCL 2/Lecture/Lecture 8/tweets.xml\")\n",
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Element 'text' at 0x10d781120>,\n",
       " <Element 'text' at 0x10d783380>,\n",
       " <Element 'text' at 0x10d783b00>,\n",
       " <Element 'text' at 0x10d781bc0>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = tree.findall(\"./tweet/text\") # find the <text> tags nested in a <tweet> tag\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aâ¿ + bâ¿ = câ¿ for n > 2'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[-1].text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing XML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = ET.Element(\"examples\")\n",
    "ET.SubElement(root, \"example\")\n",
    "subelement = ET.SubElement(root, \"example\", id=\"123\")\n",
    "subelement.text = \"Content! <:-)\"\n",
    "tree = ET.ElementTree(root)\n",
    "with open(\"new.xml\", \"wb\") as outfile:\n",
    "\ttree.write(outfile, xml_declaration=True, encoding=\"utf-8\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Extracting information from XML\n",
    "\n",
    "[*The ArchiMob Corpus*](https://www.spur.uzh.ch/en/departments/research/textgroup/ArchiMob.html) is a collection of transcribed texts in Swiss German."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the XML file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('archimob_1044.xml', <http.client.HTTPMessage at 0x10d2e2bd0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "urlretrieve(\"https://drive.switch.ch/index.php/s/vYZv9sNKetuPYTn/download?path=%2F&files=1044.xml\", \"archimob_1044.xml\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the XML file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "tree = ET.parse('archimob_1044.xml')\n",
    "\n",
    "# Use TEI as default namespace (without prefix)\n",
    "ns = {\"\": \"http://www.tei-c.org/ns/1.0\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transcription 1044'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Find the <title> element, and get its text\n",
    "tree.find(\"./teiHeader//title\", ns).text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the longest noun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w0/vj591k5d68b56bmdjn3jxdk00000gn/T/ipykernel_1905/3385098107.py:1: FutureWarning: This search is broken in 1.3 and earlier, and will be fixed in a future version.  If you rely on the current behaviour, change it to \".//w[@tag = 'NN']\"\n",
      "  noun = (element.text for element in tree.findall(\"//w[@tag = 'NN']\", ns))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kholleggtiivsaueggsischtÃ¤nz'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun = (element.text for element in tree.findall(\"//w[@tag = 'NN']\", ns))\n",
    "\n",
    "max(noun, key= len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ca2c2cbbbe4bab7a06683c64b1b96155c093f68e59474fbda709036a7610f06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
