{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrkDic2dAWRv"
   },
   "source": [
    "#Exercise 3: A Simple Classifier, Optimization and BERT.\n",
    "\n",
    "You can work in pairs or individually.\n",
    "\n",
    "Upload your solution on OLAT before the deadline: **Friday, 22nd November 2024,  at 12:15**.\n",
    "\n",
    "If you have any questions, post them on OLAT.\n",
    "\n",
    "**Submission Format**\n",
    "- Filename: **olatnameStudent1_olatnameStudent2_ml_ex2.ipynb**\n",
    "- Include the names of **both team members** in the block below.\n",
    "- If you have multiple files, place your file(s) in a compressed folder (zip).\n",
    "\n",
    "\n",
    "Good luck! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tt-GBpM8A44D"
   },
   "source": [
    "Mert Erol, 20-915-245, merol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMxI0N0feLlR"
   },
   "source": [
    "**Important: only change the code where you're asked to (TODO). Please don't change the rest of the code.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bszxM6Hb0M1U"
   },
   "source": [
    "# Task 1. Training a Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMdpvgLgS84_"
   },
   "source": [
    "In this task you will train a simple classifier using a feed-forward network. Go through the code, finish it where necessary (TODO) and answer the questions in code comments and in the text cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4yxae_b2gnP"
   },
   "outputs": [],
   "source": [
    "#import all the necessary libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TP-ES7mr3udh"
   },
   "outputs": [],
   "source": [
    "#here we will load the Iris dataset: a dataset that contains information about 3 kinds of Iris flowers\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10o9XYGG4E85"
   },
   "outputs": [],
   "source": [
    "#always remember to split your data into at least 2 sets: train and test\n",
    "\n",
    "#TODO: set test size to 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=..., random_state=42)\n",
    "\n",
    "#we will also standardize the data to improve model convergence\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#and now we convert our data into PyTorch tensors\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeImdKrjTo9_"
   },
   "source": [
    "Model equations:\n",
    "\n",
    "- $\\mathbf{h} = \\mathbf{\\sigma(Hx)}$\n",
    "\n",
    "- $\\mathbf{out} = \\mathbf{Oh}$\n",
    "\n",
    "where $\\mathbf{\\sigma} = \\mathbf{ReLU}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xA33XjFF6RWz"
   },
   "outputs": [],
   "source": [
    "#here we define the model\n",
    "\n",
    "class Iris(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Iris, self).__init__()\n",
    "    self.H = nn.Linear(4, 10)\n",
    "    self.O = nn.Linear(10, 3)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    #TODO: define the forward pass\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozLQ0RvK69wp"
   },
   "outputs": [],
   "source": [
    "#instantiate the model, the loss function and an optimizer\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model = ... #TODO\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21-ctoCFKbty"
   },
   "outputs": [],
   "source": [
    "#now we create a dataset using data loader from PyTorch\n",
    "\n",
    "dataset_size = len(X_train_tensor)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=dataset_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hqYDacoB7PWu"
   },
   "outputs": [],
   "source": [
    "#train the model\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "loss_values = [] #we will store the values here so that we can visualize them later\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  epoch_loss = 0\n",
    "\n",
    "  for inputs, labels in train_loader:\n",
    "\n",
    "    optimizer.zero_grad() #TODO: what does this line do? Why do we need it?\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_function(outputs, labels)\n",
    "\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "    #here we perform the backward pass\n",
    "\n",
    "    loss.backward() #TODO: What does this line do?\n",
    "    optimizer.step() #TODO: What does this line do?\n",
    "\n",
    "  average_loss = epoch_loss / len(train_loader)\n",
    "  loss_values.append(average_loss)\n",
    "\n",
    "  #we will print the loss so we can see how it progresses\n",
    "  print(f'Epoch: {epoch}/{epochs},\\nLoss: {loss.item()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ns_5PHsyOSiZ"
   },
   "outputs": [],
   "source": [
    "#TODO: visualise the progression of loss using a line plot (loss per epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgrc1mE3Vjq0"
   },
   "source": [
    "TODO: answer the following questions.\n",
    "\n",
    "- Look at the cell where we create a data loader. Which optimization algorithm are we using: GD, SGD or mini-batch GD? How do you know?\n",
    "\n",
    "- Observe how the loss value changes. Considering the optimizer, do you notice any trends? How do these trends align with the expected behaviour of the optimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5ra3tF40pHu"
   },
   "source": [
    "# Task 2. Adjusting Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqS-Os4cXiz4"
   },
   "source": [
    "In this task you will experiment with **the same model** setup, but **different hyperparameters** to observe their effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yKC1g-Y02VI"
   },
   "source": [
    "## Task 2.1. Adjusting Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpsM7igoX9Jw"
   },
   "source": [
    "In task 1 you used one of the following optimizing algorithms: GD, SGD or mini-batch GD.\n",
    "\n",
    "Your task is:\n",
    "\n",
    "- to **finish the code** in the cells below and implement the other **2 optimizers** (e.g., if you say that task 1 uses SGD, you have to implement GD and mini-batch GD);\n",
    "\n",
    "- to answer the questions (comments in code and text cell at the end of the task);\n",
    "\n",
    "- to visualise loss (you can reuse your code from task 1).\n",
    "\n",
    "\n",
    "\n",
    "Note #1: don't be intimidated by all the code - it's the same code as in task 1, you just need to adjust it wherever there's a 'TODO'.\n",
    "\n",
    "Note #2: To implement different optimizers, you just need to set the right batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eeeoxLBdKIM5"
   },
   "outputs": [],
   "source": [
    "#TODO: reinstantiate the model, the loss function and the optimizer. Why do we need to do it?\n",
    "\n",
    "torch.manual_seed(42) #TODO: What does this line of code do?\n",
    "\n",
    "model = ... #TODO\n",
    "loss_function = ... #TODO\n",
    "optimizer = ... #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W3jajBN3KlAt"
   },
   "outputs": [],
   "source": [
    "#now we create a dataset using data loader from PyTorch\n",
    "\n",
    "dataset_size = len(X_train_tensor)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=..., shuffle=True) #TODO: adjust the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYGHqOQ_KLwy"
   },
   "outputs": [],
   "source": [
    "#train the model\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "loss_values = [] #we will store the values here so that we can visualize them later\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  epoch_loss = 0\n",
    "\n",
    "  for inputs, labels in train_loader:\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_function(outputs, labels)\n",
    "\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "    #TODO: perform the backward pass\n",
    "\n",
    "    ...\n",
    "    ...\n",
    "\n",
    "  average_loss = epoch_loss / len(train_loader)\n",
    "  loss_values.append(average_loss)\n",
    "\n",
    "  #we will print the loss so we can see how it progresses\n",
    "  print(f'Epoch: {epoch}/{epochs},\\nLoss: {loss.item()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-COJzWSZIRG"
   },
   "outputs": [],
   "source": [
    "#visualise the loss - you can reuse your code from task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfhD_8AVZaIb"
   },
   "source": [
    "Now repeat the process for the 3rd optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "foOonRgGZhpE"
   },
   "outputs": [],
   "source": [
    "#TODO: reinstantiate the model, the loss function and the optimizer.\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model = ... #TODO\n",
    "loss_function = ... #TODO\n",
    "optimizer = ... #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PqAH3kKNZhpF"
   },
   "outputs": [],
   "source": [
    "#now we create a dataset using data loader from PyTorch\n",
    "\n",
    "dataset_size = len(X_train_tensor)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=..., shuffle=True) #TODO: adjust the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JY7DdRsgZhpF"
   },
   "outputs": [],
   "source": [
    "#train the model\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "loss_values = [] #we will store the values here so that we can visualize them later\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  epoch_loss = 0\n",
    "\n",
    "  for inputs, labels in train_loader:\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_function(outputs, labels)\n",
    "\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "    #TODO: perform the backward pass\n",
    "\n",
    "    ...\n",
    "    ...\n",
    "\n",
    "  average_loss = epoch_loss / len(train_loader)\n",
    "  loss_values.append(average_loss)\n",
    "\n",
    "  #we will print the loss so we can see how it progresses\n",
    "  print(f'Epoch: {epoch}/{epochs},\\nLoss: {loss.item()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrSE_8ycZhpF"
   },
   "outputs": [],
   "source": [
    "#visualise the loss - you can reuse your code from task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHWpqfMOZldE"
   },
   "source": [
    "TODO: answer the following questions.\n",
    "\n",
    "- Observe the loss values for all 3 optimizers (including the one in task 1). What trends do you observe?\n",
    "\n",
    "- How does changing the batch size impact the training process? What is the trade-off between using a smaller batch size versus a larger one in terms of training time and model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkZ-5XNt06E0"
   },
   "source": [
    "## Task 2.2. Adjusting the Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wsRN4rZgqGB"
   },
   "source": [
    "In this part of the task you will experiment with the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHoC3C7pntBa"
   },
   "source": [
    "Finish the training function below. It collects and returns loss values for a given learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iW6WqsQvR9t2"
   },
   "outputs": [],
   "source": [
    "def train_model(learning_rate):\n",
    "\n",
    "  torch.manual_seed(42)\n",
    "  model = Iris()\n",
    "  loss_function = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "  train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "  epochs = 50\n",
    "  loss_values = []\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "      epoch_loss = 0\n",
    "      for inputs, labels in train_loader:\n",
    "\n",
    "          #TODO: perform the forward pass\n",
    "          ...\n",
    "          #TODO: compute loss\n",
    "          ...\n",
    "          epoch_loss += loss.item()\n",
    "          #TODO: perform the backward pass\n",
    "          ...\n",
    "\n",
    "\n",
    "      average_loss = epoch_loss / len(train_loader)\n",
    "      loss_values.append(average_loss)\n",
    "\n",
    "  return loss_values\n",
    "\n",
    "#TODO: collect and observe loss values with different learning rates. Make sure you have data for high and low learning rates!\n",
    "learning_rates = [...]\n",
    "loss_history = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    loss_history[lr] = train_model(lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foaUOCMvq0ZC"
   },
   "source": [
    "TODO: plot the loss per epoch for different learning rates:\n",
    "\n",
    "- create a single plot figure;\n",
    "\n",
    "- for each learning rate, plot the loss values against the epochs;\n",
    "\n",
    "- make sure that each learning rate is represented by a distinct line: use different colours or markers to differentiate between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XdHN0FCSrNJU"
   },
   "outputs": [],
   "source": [
    "#TODO: code to visualize the losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kA4QlT9QrSEg"
   },
   "source": [
    "TODO: answer the question\n",
    "\n",
    "- Observe the plot. What trends do you notice? What differences do you notice? Can you explain them?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlAvB6w71CPE"
   },
   "source": [
    "## Task 2.3. Adjusting Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krMoRPTuopYy"
   },
   "source": [
    "Three tasks:\n",
    "1. Finish the code block \"Model with dropout\"\n",
    "2. Change the drop out rate, observe the patterns, and answer what is going on in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3__NlPEmdSq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-w0i4F5uUWHE"
   },
   "outputs": [],
   "source": [
    "# nothing to change here\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "def create_noisy_dataset(n_samples=1000):\n",
    "    X = np.random.randn(n_samples, 50)  # 100 noisy features\n",
    "    # true pattern: only first five features\n",
    "    y = (2 * X[:, 0] + X[:, 1] - 1.5 * X[:, 2] + 0.5 * X[:, 3] - X[:, 4] > 0).astype(int)\n",
    "    # add noise to labels (15% random flip in this binary classification task)\n",
    "    noise_mask = np.random.random(n_samples) < 0.15\n",
    "    y[noise_mask] = 1 - y[noise_mask]\n",
    "    return torch.FloatTensor(X), torch.LongTensor(y)\n",
    "\n",
    "# create dataset\n",
    "X, y = create_noisy_dataset(2000)  # 2000 samples\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLBJwPftm-ay"
   },
   "outputs": [],
   "source": [
    "# add dropout layers here\n",
    "\n",
    "# Model with dropout\n",
    "# read this: https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\n",
    "class ModelWithDropout(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(ModelWithDropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(50, 128)\n",
    "        self.dropout1 = ## TOOO\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.dropout2 = ## TOOO\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.dropout3 = ## TOOO\n",
    "        self.fc4 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        return self.fc4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_sYnLQfnJNa"
   },
   "outputs": [],
   "source": [
    "# Model without dropout\n",
    "class ModelWithoutDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelWithoutDropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(50, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        return self.fc4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTvNkHyMnCy6"
   },
   "outputs": [],
   "source": [
    "# nothing to change here\n",
    "def train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=200):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # train accuracy\n",
    "            train_preds = torch.argmax(model(X_train), dim=1)\n",
    "            train_acc = (train_preds == y_train).float().mean().item()\n",
    "\n",
    "            # test accuracy\n",
    "            test_preds = torch.argmax(model(X_test), dim=1)\n",
    "            test_acc = (test_preds == y_test).float().mean().item()\n",
    "\n",
    "            # collect the accuracies for plotting\n",
    "            train_accs.append(train_acc)\n",
    "            test_accs.append(test_acc)\n",
    "\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f'Epoch {epoch+1}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "    return train_accs, test_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbMmAF35pq8c"
   },
   "outputs": [],
   "source": [
    "# nothing to change in this block\n",
    "def plot_training_results(train_acc_dropout, test_acc_dropout, train_acc_no_dropout, test_acc_no_dropout, dropout_rate):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # plot model with dropout\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_acc_dropout, 'b-', label='Train (with dropout)', alpha=0.7)\n",
    "    plt.plot(test_acc_dropout, 'r-', label='Test (with dropout)', alpha=0.7)\n",
    "    plt.title(f'Model with Dropout (rate={dropout_rate})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.ylim(0.4, 1.0)\n",
    "\n",
    "    # plot model without dropout\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_acc_no_dropout, 'b-', label='Train (no dropout)', alpha=0.7)\n",
    "    plt.plot(test_acc_no_dropout, 'r-', label='Test (no dropout)', alpha=0.7)\n",
    "    plt.title('Model without Dropout')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.ylim(0.4, 1.0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YcctDcELqYw7"
   },
   "outputs": [],
   "source": [
    "# you can play around with this dropout rate\n",
    "my_dropout_rate = 0.5\n",
    "\n",
    "# nothing to change below\n",
    "print(\"Training model with dropout...\")\n",
    "model_with_dropout = ModelWithDropout(dropout_rate=my_dropout_rate)\n",
    "train_acc_dropout, test_acc_dropout = train_and_evaluate(\n",
    "    model_with_dropout, X_train, y_train, X_test, y_test\n",
    ")\n",
    "\n",
    "print(\"\\nTraining model without dropout...\")\n",
    "model_without_dropout = ModelWithoutDropout()\n",
    "train_acc_no_dropout, test_acc_no_dropout = train_and_evaluate(\n",
    "    model_without_dropout, X_train, y_train, X_test, y_test\n",
    ")\n",
    "\n",
    "plot_training_results(\n",
    "   train_acc_dropout,\n",
    "   test_acc_dropout,\n",
    "   train_acc_no_dropout,\n",
    "   test_acc_no_dropout,\n",
    "   dropout_rate=my_dropout_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnYxJ1ITqotO"
   },
   "source": [
    "**TODO:**\n",
    "\n",
    "What do you observe when you set my_dropout_rate to 0.5 in the plot? Explain what happened in both models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HA0mfsSrpke"
   },
   "source": [
    "# Task 3: Huggingface and transformers library\n",
    "\n",
    "Goals:\n",
    "- Test out an interactive inference API on HF ðŸ¤—\n",
    "- Import a model from hugging face\n",
    "- define the model and perform a NLP task\n",
    "- experiment with its capabilities and possible biases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDDsMT2MysRL"
   },
   "source": [
    "## Task 3.1 Huggingface: inference API\n",
    "\n",
    "Follow the Link to view the model on Huggingface ðŸ¤—: https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment\n",
    "\n",
    "For more Information about the model and its intended use and performance, check out the corresponding paper: https://arxiv.org/abs/2104.12250\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JxYsa0ilLrp"
   },
   "source": [
    "**TODO: Answer the following Questions:**\n",
    "\n",
    "a) What kind of data was the model trained on, and for which subgroup of languages was sentiment-finetuning performed?\n",
    "\n",
    "Trained on: tweets\n",
    "Finetuned on: 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt)\n",
    "\n",
    "b) You have the Sentence: **\"My cat had kittens\"** and you want test how the sentiment lable is influenced by including Punctuation **(.,?,!)**. You can use the Inference API on the website to get your outputs. Further more you are also curious what effect **different emojis** at the end of the sentence have on the model output, find 3 interesting examples and note them down as well.\n",
    "  -   My cat had kittens --> Neutral: 0.676\n",
    "  -   My cat had kittens. --> Neutral: 0.693\n",
    "  -   My cat had kittens â™¥ -->\n",
    "  -   My cat had kittens ðŸ˜ðŸ˜™ðŸ˜™ -->\n",
    "  -   My cat had kittens ðŸ˜° -->\n",
    "  -   ...\n",
    "  -   ...\n",
    "\n",
    "\n",
    "Your interpretation:\n",
    "\n",
    "c) Find one or two interesting examples on your own and describe your results and why you thought they were noteworthy (if you choose a language different than english or german, we would appreciate a translation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Yuth4WM5Lbs"
   },
   "source": [
    "## Task 3.2 Huggingface: import a model and use it locally.\n",
    "\n",
    "- We now know how to find a model on Huggingface and how to interact with the inference API (if available). But how can we load the model locally and actually work with it? There are 2 main approaches, using the pipeline or loading a model directly. We will have a closer look at both appraoches below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02WKK9sd0FLw"
   },
   "outputs": [],
   "source": [
    "# first we need to have all necessary libraries installed\n",
    "!pip install transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1PWOc6Ywwlv"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, XLMRobertaForSequenceClassification, pipeline\n",
    "from scipy.special import softmax\n",
    "from transformers import AutoConfig\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# define model variable with path to the model in Huggingface\n",
    "model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "\n",
    "# define Model and Tokenizer you want to use\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z_MW71Q9BvL"
   },
   "source": [
    "### 3.2.1 Using the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vaa0NkdJ3Yux"
   },
   "outputs": [],
   "source": [
    "# No change is needed for this block\n",
    "# Create the sentiment analysis pipeline with the chosen tokenizer and model\n",
    "sentiment_task = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Analyze sentiment for a given text\n",
    "result = sentiment_task(\"Huggingface es lo mejor! Awesome library ðŸ¤—ðŸ˜Ž\")\n",
    "\n",
    "# Print top label and score\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JqFwk84ddAE"
   },
   "source": [
    "**TODO**: What is the advantage of the transformer pipeline?\n",
    "\n",
    "Simple, automatic, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXmMjh_R9JJH"
   },
   "source": [
    "### 3.2.2 Apply tokenizer and model step by step\n",
    "\n",
    "Find the hugging face quick tour here: https://huggingface.co/docs/transformers/en/quicktour\n",
    "\n",
    "On the top right corner, there is a colab link - choose pytorch and open it. Or, the link is here: https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/quicktour.ipynb\n",
    "\n",
    "There are also other languages! It is on the top left corner, next to the versions.\n",
    "\n",
    "The videos from Huggingface are also very helpful.\n",
    "\n",
    "If you find the problems below are difficult to solve, check out these resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roH9ffM91e-c"
   },
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_path)\n",
    "# Tokenize the input str.\n",
    "inputs = tokenizer(\"Huggingface es lo mejor! Awesome library ðŸ¤—ðŸ˜Ž\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDxB7PZUlZoT"
   },
   "source": [
    "**Fill in six TODO blocks below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IxcICjP4ex9Q"
   },
   "outputs": [],
   "source": [
    "## TODO_1: print out the input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iIuhO-2jezyz"
   },
   "outputs": [],
   "source": [
    "## TODO_2: convert the input_ids back to text and print out the first result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffB2rZORe1EY"
   },
   "outputs": [],
   "source": [
    "## TODO_3: Tokenize a new word \"Huggingface\" (or any word you like)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TiMpn9VXe-Zl"
   },
   "outputs": [],
   "source": [
    "## TODO_4: What's the input_id \"0\" and \"2\" refering to? Find them using code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UG5OsDR7fCLJ"
   },
   "outputs": [],
   "source": [
    "# TODO_5: perform forward pass, and check the what the output looks like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4IZAJE8MfxDw"
   },
   "outputs": [],
   "source": [
    "def show_scores(output):\n",
    "  scores = output[0][0].detach().numpy()\n",
    "\n",
    "  ## TODO_6: describe the outputs for \"scores\", the first \"ranking\", and the second \"ranking\":\n",
    "  scores = softmax(scores)\n",
    "  ## TODO\n",
    "  ranking = np.argsort(scores)\n",
    "  ## TODO\n",
    "  ranking = ranking[::-1]\n",
    "  ## TODO\n",
    "\n",
    "  for i in range(scores.shape[0]):\n",
    "      l = config.id2label[ranking[i]]\n",
    "      s = scores[ranking[i]]\n",
    "      print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n",
    "\n",
    "show_scores(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPfObBa9dzE0"
   },
   "source": [
    "**TODO**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKxj_iv8eCgU"
   },
   "source": [
    "Task 1: finish TODO_1, TODO_2, TODO_3, TODO_4, TODO_5, TODO_6 in the code above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FS3u9g5aeDTs"
   },
   "source": [
    "Task 2: What is the advantage/disadvantage of this approach (compared to task 4.2.1)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Riek0K7rGR07"
   },
   "source": [
    "## 3.3 Huggingface: Fill-Mask-Task\n",
    "For different tasks you might want to use different models. In the previous task we looked into SequenceClassification or more specifically \"sentiment analysis\". In this task we look at the \"Fill-Mask\" Task, where a model predicts the masked word. Documentation of ForMaskedLM: https://huggingface.co/docs/transformers/tasks/masked_language_modeling\n",
    "\n",
    "- For more details about the model, you can visit the model page on HF: https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5dSiWsRJYbc"
   },
   "source": [
    "**TODO**\n",
    "\n",
    "Write your own example sentence. Then we already provided you with a base code for your task and you can **choose one of the two approaches** (pipeline or step by step application).\n",
    "\n",
    "**You are allowed to change/adapt the code, as long as the output is presented the way it's described in 3.**\n",
    "\n",
    "Your task is to adapt the code so that:\n",
    "\n",
    "1. You define **3 different versions** of your original sentence, each version masks a different word/token.\n",
    "\n",
    "2. loop over each sample sentence and print out the **top 5** results for you masked word in each input sentence.\n",
    "\n",
    "3. your output sould show all results **at once** (e.g. show input + top 5 results incl. probability scores, show input_2 + top 5 results incl. probability scores, etc.). It doen't need to look pretty, as long as its functional and easy to read.\n",
    "\n",
    "4. What do you think about the results?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSlbYuPeFcJ2"
   },
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IyutJsQMSCrx"
   },
   "source": [
    "### Approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNI-u-BEJknW"
   },
   "outputs": [],
   "source": [
    "#Todo: Use your own sentence here\n",
    "text = \"ðŸ”¥The goal of life is <mask> . ðŸ”¥\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "logits = model(**inputs).logits\n",
    "mask_token_logits = logits[0, mask_token_index, :]\n",
    "\n",
    "#Todo: Change the code so that only the top 3 results are displayed\n",
    "softmax_output = torch.nn.functional.softmax(mask_token_logits, dim=-1)\n",
    "sorted_output = softmax_output.sort(descending=True)\n",
    "top_3_results = torch.topk(mask_token_logits, 5, dim=1)\n",
    "\n",
    "\n",
    "print(text, \"\\n\")\n",
    "for i in range(len(top_3_results.indices[0])):\n",
    "    token = top_3_results.indices[0][i].item()\n",
    "    probability = softmax_prob[i]\n",
    "    filled_text = text.replace(tokenizer.mask_token, tokenizer.decode([token]))\n",
    "    print(f\"Filled text: {filled_text}, Probability: {probability:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V86V2ZYXSKBD"
   },
   "source": [
    "### Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40ronK93HFbK"
   },
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lauB3XfuHr4k"
   },
   "outputs": [],
   "source": [
    "#Todo: Use your own sentence here\n",
    "results = pipe(\"ðŸ”¥The goal of life is <mask>.ðŸ”¥\")\n",
    "#Todo: Change the code so that only the top 3 results are displayed\n",
    "print(text, \"\\n\" )\n",
    "for res in results:\n",
    "  print(res)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
