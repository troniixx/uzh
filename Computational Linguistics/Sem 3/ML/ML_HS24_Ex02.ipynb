{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQJiiyqZe0a2"
      },
      "source": [
        "# Exercise 2: Pytorch\n",
        "\n",
        "You can work in pairs or individually.\n",
        "\n",
        "Upload your solution on OLAT before the deadline: **Friday, 8th November 2014,  at 12:15**.\n",
        "\n",
        "If you have any questions, post them on OLAT.\n",
        "\n",
        "**Submission Format**\n",
        "- Filename: **olatnameStudent1_olatnameStudent2_ml_ex2.ipynb**\n",
        "- Include the names of **both team members** in the block below.\n",
        "- If you have multiple files, place your file(s) in a compressed folder (zip).\n",
        "\n",
        "\n",
        "Good luck! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q1Dw1XrfJUg"
      },
      "source": [
        "---\n",
        "Group Members:\n",
        "\n",
        "**TODO**: Your names, your short names, and student numbers\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Bhgg_3JM6ur"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEgs5e_qfVYH"
      },
      "source": [
        "## Task 1: Tensor Manipulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28XdcX6gip-k"
      },
      "source": [
        "### Task 1.1 Squeeze and Unsqueeze\n",
        "- The first task: for the first three blocks, explain ONLY squeeze and unsqueeze dimension in SIMPLE and SHORT sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zf8TO5SJbvEi"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(2, 1, 4, 1, 3)\n",
        "x = torch.squeeze(x)\n",
        "# TODO: What is x.shape? Explain what happened\n",
        "# squeeze removes dimensions of size 1 from the shape of the tensor\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mMtCG_pbxn2"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(3, 4)\n",
        "x = torch.unsqueeze(x, 0) # add a dimension of size to the shape at index 0\n",
        "x = torch.unsqueeze(x, -1) # add a dimension of size to the shape at the last index\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydXbBAUpb35M"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(2, 3, 1, 4)\n",
        "x = torch.squeeze(x, 2) # remove dimension of size 1 at index 2\n",
        "x = torch.unsqueeze(x, 1) # add dimension of size 1 to the shape at index 1\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op9GztaFb_3F"
      },
      "source": [
        "- The second task is to use squeeze and unsqueeze functions to achieve the target dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfoGL754b4DY"
      },
      "outputs": [],
      "source": [
        "# Starting shape: (3, 1, 4, 1)\n",
        "# Target shape: (1, 3, 4, 1, 1)\n",
        "x = torch.randn(3, 1, 4, 1)\n",
        "x = torch.squeeze(x, 1)\n",
        "x = torch.unsqueeze(x, 0)\n",
        "x = torch.unsqueeze(x, -1)\n",
        "\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "legbljCDqkAU"
      },
      "source": [
        "### Task 1.2 Batch Matrix Multiplication\n",
        "Given the following operations involving batch matrix multiplication (BMM)\n",
        "- fill in the code (the correct dimension for d)\n",
        "- answer questions.\n",
        "\n",
        "You can find information about batch matrix multiplication here:\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.bmm.html\n",
        "\n",
        "https://stackoverflow.com/questions/50826644/why-do-we-do-batch-matrix-matrix-product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyREfDMJcOxr"
      },
      "outputs": [],
      "source": [
        "# Mini-batch size of 64\n",
        "a = torch.randn(64, 128, 256)   # [batch_size, seq_length, embedding_dim]\n",
        "b = torch.randn(64, 256, 512)   # [batch_size, embedding_dim, hidden_dim]\n",
        "\n",
        "# First BMM operation\n",
        "x = torch.bmm(a, b)             # TODO: What's the shape? Calculate it!\n",
        "print(\"shape of x: \", x.shape)\n",
        "\n",
        "# Create tensor d with correct dimensions\n",
        "d = torch.randn(64, 512, 9)   # TODO: Fill in the correct dimensions\n",
        "print(\"shape of d: \", d.shape)\n",
        "\n",
        "# Second BMM operation\n",
        "y = torch.bmm(x, d)             # Target shape: [64, 128, 9]\n",
        "                                # (batch_size, seq_length, output_dim)\n",
        "print(\"shape of y: \",y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rga3vt3icPp9"
      },
      "source": [
        "TODO:\n",
        "1. What is the shape of intermediate tensor x?\n",
        "2. Fill in the shape of tensor d. Explain why you did so according to bmm.\n",
        "3. Think: Why does this sequence of operations make sense in the context of\n",
        "   deep learning (hint: think about sequence processing)? Then guess and answer: what might seq_length and embedding_dim mean? How about the output dimension of y? (Points will be given for all guesses that are relevant to Text/Speech/Images/Sciences etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZUhBTxTqrF_"
      },
      "source": [
        "###Task 1.3: Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvdP2BaKFeNi"
      },
      "source": [
        "Take a closer look at the code below, finish it (TODO) and then answer the questions. You can add print statements to get insight into the individual steps and output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CClnoyZxFjqZ"
      },
      "outputs": [],
      "source": [
        "# We have a dataset of 10 tokens, and each token is represented by a vector with a dimensionality of 5.\n",
        "vocab_size = ... #TODO\n",
        "embedding_dim = ... #TODO\n",
        "\n",
        "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "x = torch.LongTensor([1, 3, 5, 7, 9])\n",
        "\n",
        "embedded_x = embedding_layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izMr6-0tI23R"
      },
      "source": [
        "TODO: Answer the following questions:\n",
        "- a) What is the purpose of the embedding layer in neural networks?\n",
        "- b) What is the role of 'vocab_size' and 'embedding_dim' in the 'nn.Embedding' layer?\n",
        "- c) What happens if you try to embed a category that is out of the range defined by vocab_size in the nn.Embedding layer? Give an example of an out of range input value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBK-tWE4M8dH"
      },
      "source": [
        "## Task 2: Network Parameters\n",
        "\n",
        "Imagine you're training a neural network for a 10-class classification problem  that takes an input vector of size 236. You implement 2 hidden layers with 472 neurons each. You don't learn any bias terms.\n",
        "<br>\n",
        "\n",
        " 1) How many different parameters does the neural network contain? TODO\n",
        "\n",
        "<br> <br>\n",
        "2) What's the difference between parameters and hyper-parameters? Name 3 different hyper-parameters and explain their role inside a neural network. TODO\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPfZrlK0q0uZ"
      },
      "source": [
        "## Task 3: Pytorch Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeInKnh4rpRG"
      },
      "source": [
        "###Task 3.1: Realize the following model in PyTorch:\n",
        "  - [ ] Realize matrices as linear layers.\n",
        "  - [ ] Create an instance of your model.\n",
        "  - [ ] Apply it to $\\mathbf{x}$, defined in the second code cell.\n",
        "  - [ ] Determine the Cross-Entropy loss.\n",
        "\n",
        "**Model Equations:**\n",
        "\n",
        "- $\\mathbf{e}=\\mathbf{E}\\mathbf{x}$\n",
        "- $\\mathbf{h}=\\sigma(\\mathbf{W}\\mathbf{e}+\\mathbf{b})$\n",
        "- $\\mathbf{z}=\\mathbf{U}\\mathbf{h}$\n",
        "- $\\hat{\\mathbf{y}}=\\mathbf{z}$\n",
        "\n",
        "**Details:**\n",
        "\n",
        "- We are performing a classification task with **4 classes**.\n",
        "- $\\mathbf{x}$ is a batch of 4 integers that represent some tokens.\n",
        "- $\\mathbf{E}$ is a 8 x 6 embedding layer, realized with PyTorch functionality.\n",
        "- $\\mathbf{W}$ and $\\mathbf{U}$ are weight matrices. Both layers learn bias terms. The output dimension of $\\mathbf{W}$ is 10.\n",
        "- Activation function $\\sigma$ is ReLU.\n",
        "- Model output is 4 logits per token (raw values)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lee4dwrAsHrz"
      },
      "source": [
        "Note: Don't change any of the existing code, only add your solutions where it's indicated (TODO statements). Print statements can be placed wherever you need them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMCgn6tNsKTU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(..., ...) #TODO\n",
        "        self.W = nn.Linear(..., ..., ...)#TODO\n",
        "        self.U = nn.Linear(..., ..., ...) #TODO\n",
        "        self.relu = ... #TODO\n",
        "\n",
        "    def forward(self,x):\n",
        "        #TODO: ... (as many lines as you need)\n",
        "\n",
        "\n",
        "        return  #TODO:...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11x7OafSs7VG"
      },
      "outputs": [],
      "source": [
        "X=torch.tensor([1,2,3,7],dtype=torch.long)\n",
        "y=torch.tensor([1,0,1,3],dtype=torch.long)\n",
        "\n",
        "model = ... #TODO\n",
        "output = ... #TODO\n",
        "print(f'model output: {output}')\n",
        "\n",
        "\n",
        "#TODO: determine the loss\n",
        "\n",
        "loss_function = ... #TODO\n",
        "loss = ... #TODO\n",
        "print(f'loss: {loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "808ndtJeAdGZ"
      },
      "source": [
        "### Task 3.2\n",
        "\n",
        "Take another look at the cells above.\n",
        "\n",
        "- Is it a binary or a multiclass classification task? TODO\n",
        "- Why is there no activation function applied to the output? (Hint: check the documentation for Cross Entropy Loss) TODO\n",
        "- What is the vocabulary size of this model? TODO\n",
        "- What do the numbers in $\\mathbf{y}$ represent? TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESjPGt7_BO4a"
      },
      "source": [
        "## Task 4 (Bidirectional Self-)Attention\n",
        "Given a simple sentence \"he likes cats\" and the following weight matrices:\n",
        "- Weight matrix for Query ($W_q$)\n",
        "- Weight matrix for Key ($W_k$)\n",
        "\n",
        "Dimensionalities: (important!)\n",
        "- the dimensionality of input is [sentence length, d]\n",
        "- d stands for hidden embeddings\n",
        "- the dimensionality of weight matrices should be [d, d_k]\n",
        "- **from above, you can see that d_k has nothing to do with input x.**\n",
        "\n",
        "You have two tasks.\n",
        "- Follow the steps to fill in the code - calculate the weight matrix.\n",
        "- Look at the weight matrix and explain it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppuPNhU_BQCk"
      },
      "outputs": [],
      "source": [
        "# Don't change this block, just read and run\n",
        "# Weight matrices\n",
        "Wq = torch.tensor([[0.5, 2.0],\n",
        "                   [3.0, 1.5]])\n",
        "\n",
        "Wk = torch.tensor([[1.0, 0.4],\n",
        "                   [0.6, 1.2]])\n",
        "\n",
        "# Word embeddings for \"he likes cats\"\n",
        "x = torch.tensor([[1.0, 2.0],  # \"he\"\n",
        "                  [2.0, 1.0],  # \"likes\"\n",
        "                  [1.5, 1.5]]) # \"cats\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CEDMwOwdDSo"
      },
      "source": [
        "Follow the steps to compute attention weights and interpret the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Op78EGaWdCup"
      },
      "outputs": [],
      "source": [
        "# Step 1: calculate Q and K\n",
        "Q = # TODO\n",
        "K = # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHcPA8KbdG0J"
      },
      "outputs": [],
      "source": [
        "# Step 2: Compute attention scores\n",
        "# hint1: d_k has nothing to do with d_x, but instead, it is related to the weight matrix k.\n",
        "# hint2: to compute the scores, you need to transpose K to compute the scores)\n",
        "d_k = # TODO\n",
        "scores = # TODO\n",
        "scores_scaled = #TODO, here you divide the scores by ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFGKKcArUIKe"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F63v0hHudIIK"
      },
      "outputs": [],
      "source": [
        "# Step 3.1:\n",
        "# It is always a good habitude to read documentation. Always read documentation.\n",
        "# Find the documentation in torch on how you can calculate the variance of a tensor, and paste the link here\n",
        "\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paA8rDPxdK5a"
      },
      "outputs": [],
      "source": [
        "# Step 3.2:\n",
        "# Read the documentation\n",
        "# Use what you find in the documentation and calculate the variance of scores and scores_scaled\n",
        "# Hint: It should be a very simple code\n",
        "\n",
        "var_scores = # TODO\n",
        "var_scores_scaled = # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCFmwiJ4dLri"
      },
      "outputs": [],
      "source": [
        "# Step 3.3\n",
        "# the code below gives you the ratio of var_scores and var_scores_scaled\n",
        "ratio = var_scores / var_scores_scaled\n",
        "print(ratio)\n",
        "\n",
        "# Answer: what number the ratio is close to?\n",
        "\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Du4orBRwdOg3"
      },
      "outputs": [],
      "source": [
        "# Step 4: Apply softmax to get attention weights\n",
        "weights = # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba0JZ2rTeBgO"
      },
      "source": [
        "TODO: Answer the questions below\n",
        "\n",
        "a) What do the values in the first row of attention weights represent?\n",
        "\n",
        "b) Look at the last row of weights - what does each number tell us?\n",
        "\n",
        "c) Which row or column you should look at if you want to find out which word pays most attention to \"likes\"? and what is the word?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APvWmymY-4Sn"
      },
      "source": [
        "## Task 5: Training Optimization Problems\n",
        "\n",
        "### Task 5.1:\n",
        "\n",
        "For each scenario, choose between GD, SGD and mini-batch GD.\n",
        "\n",
        "a) You are training a deep neural network. You have a lot of training samples but limited GPU memory. Now you want to have some stability during training and enable parallel processing.\n",
        "\n",
        "**#TODO**\n",
        "\n",
        "b) You're training a model with data points coming in in real time, and you want to process them immediately.\n",
        "\n",
        "**#TODO**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvXAh2LETj4x"
      },
      "source": [
        "### Task 5.2:\n",
        "\n",
        "You're training a neural model. After several epochs you notice that the loss decreases very slowly, so you increase the learning rate. Now the loss fluctuates significantly. What were the issues (before and after changing the leaning rate)?\n",
        "\n",
        "**#TODO**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
