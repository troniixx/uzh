{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# NER and NEL with NLP Tools\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### General Instructions\n",
    "> - Run this notebook cell by cell and observe what happens.\n",
    "> - If a cell is marked with `TODO`, there is a small task for you.\n",
    "> - Report the observations in your submission doc, as requested in the assignment.\n",
    "> - Be generous with screenshots - show us your results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy\n",
    "%pip install nltk\n",
    "%pip install wikipedia-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy\n",
    ">First, we will perfom `NER (Named Entity Recognition)` and `NEL (Named Entity Linking)` with spaCy.\n",
    "><br>\n",
    "><br>SpaCy is a free and open-source python library with a lot of built-in capabilities. \n",
    "><br>It’s becoming increasingly popular for processing and analyzing data in the field of NLP, and you will soon see why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import wikipediaapi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In this section, you will see how SpaCy's `English-language` model works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the English model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE It is important that you run cells one-by-one, since you need to activate all previous cells to be able to run a next one.\n",
    "# the below lines help you download the English model \n",
    "# and load it to this notebook\n",
    "spacy.cli.download('en_core_web_sm')\n",
    "import en_core_web_sm\n",
    "nlp_english = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save your text to a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# the below string is given to you as an example.\n",
    "# substitute it with the text of your choice.\n",
    "# NOTE: triple quotations allow you to paste a multiline text\n",
    "\n",
    "text = \"\"\"\n",
    "Secretary of State Antony J. Blinken made an unannounced visit on Sunday to the Israeli-occupied West Bank to meet with Mahmoud Abbas, the president of the internationally backed Palestinian Authority, and other Palestinian leaders.\n",
    "\n",
    "The top American diplomat’s visit to the West Bank city of Ramallah followed talks with Israeli and Arab leaders in Tel Aviv and Amman, Jordan, that have focused on preventing Israel’s war against Hamas in the Gaza Strip from spreading and on convincing the Israeli government to do more to limit civilian casualties in the enclave.\n",
    "\n",
    "In Israel, Mr. Blinken had urged protections for Palestinian noncombatants and for “humanitarian pauses” in the fighting, even as he supported Israel’s right to defend itself. The Gazan Health Ministry, which is part of the Hamas-run government, says that Israeli attacks have killed more than 9,400 people in the territory, a toll that has provoked outrage around the world — and in the West Bank.\n",
    "\n",
    "Mr. Blinken and Mr. Abbas last held talks three weeks ago in Ramallah, days after Hamas extremists from Gaza launched a surprise attack that killed about 1,400 people in Israel, mostly civilians.\n",
    "\n",
    "Mr. Abbas is the leader of the Palestinian Authority, which Hamas, a rival, ousted from Gaza in a violent coup in 2007 after winning elections the previous year. Mr. Abbas has long advocated the establishment of a Palestinian state alongside Israel, and Palestinian security forces under his direction have worked closely with Israel to arrest Palestinian militants.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the input text using the SpaCy model\n",
    "# NOTE If the output data are too long to be displayed, open it in a text editor as suggested.\n",
    "\n",
    "article = nlp_english(text) # \n",
    "\n",
    "for entity in article.ents:\n",
    "    print(f'{entity.label_}\\t{entity.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the NER results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this nice feature allows you to visualize the entities\n",
    "displacy.render(article, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEL: Linking to Wikipedia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a function allowing you to link the entities to the Wikipedia database\n",
    "def entity_linking(some_text):\n",
    "    doc = nlp_english(some_text)\n",
    "    wiki_wiki = wikipediaapi.Wikipedia('en', headers={'User-Agent': 'my-custom-agent'})\n",
    "    unique_links = set() # use a set to avoid duplicates\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        \n",
    "        # TODO modify the below list so that it includes all the types of entities you have found in your text\n",
    "        if ent.label_ in ['PERSON', 'ORG']: # modify this list, e.g. add 'GPE'\n",
    "            entity_page = wiki_wiki.page(ent.text)\n",
    "            if entity_page.exists() and entity_page.fullurl not in unique_links:\n",
    "                unique_links.add(entity_page.fullurl)\n",
    "                print(f\"{ent.text}\\tWikipedia URL: {entity_page.fullurl}\")\n",
    "\n",
    "\n",
    "entity_linking(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Run the cells below to see how SpaCy copes with German text.\n",
    "><br>`TODO`: Select a short German text of your own and see how the model manages to process it.\n",
    ">><br>`NOTE`: If you are doing the `Bonus` task, substitute German with another language of your choice available at spaCy (https://spacy.io/usage/models). Try to figure out what needs to be changed when changing the language. No worries, you are not going to lose any points if you do not manage to make it run - that's what `BONUS` is for!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the German model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here you download and load the German model\n",
    "spacy.cli.download('de_core_news_sm') # NOTE modify this line if working with a different language\n",
    "import de_core_news_sm # NOTE modify this line if working with a different language\n",
    "nlp = de_core_news_sm.load() # NOTE modify this line if working with a different language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save your text to a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: substitute the string with a text of your choice.\n",
    "# It does not matter if you speak German or not, just observing the labels is enough.\n",
    "\n",
    "text = \"\"\"\n",
    "«Wer bist du?» – Prinzessin Kates Antwort sagt viel über sie aus\n",
    "Bei ihrem Besuch in Schottland wurde die Prinzessin von Wales von einem Kind gefragt, wer sie sei. Die Antwort mag überraschen, doch es steckt eine Absicht dahinter. Prinzessin Kate hat bei einem öffentlichen Auftritt einmal mehr ihre Unterstützung ihrem Ehemann und Thronfolger Prinz William gegenüber bewiesen. Bei ihrem kürzlichen Besuch in Schottland begrüssten die beiden Royals wie üblich zahlreiche Fans – auch ganz kleine vor einer Primarschule. Zunächst schwärmte Kate, die wie so oft alle Blicke auf sich zog, von der schottischen «Burghead»-Grundschule und sagte: «Ich finde die Schule ganz toll!» Dann kam es zu einer niedlichen Konversation zwischen Kate und einem Schuljungen. Dieser fragte die Prinzessin ganz unverblümt: «Wer bist du?» – eine Frage, die Kate wohl nicht alle Tage gestellt bekommt. Doch die Prinzessin reagiert, wie immer in der Öffentlichkeit, gelassen und gefasst: «Ich bin mit ihm verheiratet», sagt sie an den Jungen gewandt und zeigt dabei auf Prinz William, der unweit neben ihr steht.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = nlp(text)\n",
    "\n",
    "for entity in article.ents:\n",
    "    print(f'{entity.label_}\\t{entity.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the NER Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the NE\n",
    "displacy.render(article, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEL: Linking to Wikipedia Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this is a function allowing you to link the entities to the Wikipedia database\n",
    "\n",
    "def entity_linking(some_text):\n",
    "    \n",
    "    doc = nlp(some_text)\n",
    "\n",
    "    # NOTE make sure to change 'de' to the new language abbreviation if working with a different language\n",
    "    wiki_wiki = wikipediaapi.Wikipedia('de', headers={'User-Agent': 'my-custom-agent'}) # NOTE modify this line if working with a different language\n",
    "    unique_links = set()\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "\n",
    "        if ent.label_ in ['PER', 'LOC', 'MISC']: # TODO change this list depending on the entity types found in your text\n",
    "            entity_page = wiki_wiki.page(ent.text)\n",
    "            if entity_page.exists() and entity_page.fullurl not in unique_links:\n",
    "                unique_links.add(entity_page.fullurl)\n",
    "                print(f\"{ent.text}\\tWikipedia URL: {entity_page.fullurl}\")\n",
    "\n",
    "\n",
    "entity_linking(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# NLTK\n",
    "> Now, we will perfom `NER (Named Entity Recognition)` and `NEL (Named Entity Linking)` with NLTK.\n",
    ">\n",
    ">> \"NLTK is a leading platform for building Python programs to work with human language data. \n",
    ">> <br>It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, \n",
    ">> <br>along with a suite of text processing libraries for classification, tokenization, stemming, \n",
    ">> <br>tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, \n",
    ">> <br>and an active discussion forum.\" (https://www.nltk.org/)\n",
    ">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and load the necessary packages\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Your Text to a Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" \"\"\" # TODO paste you English text here between the triple quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part performs the NER of your text\n",
    "# NOTE that the process is different from SpaCy. Can you spot the difference?\n",
    "for sent in nltk.sent_tokenize(text):\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "        if hasattr(chunk, 'label'):\n",
    "            print(chunk.label(), ' '.join(c[0] for c in chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEL: Linking to Wikipedia Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here, we define a function to both extract NE and link them to the Wikipedia database\n",
    "\n",
    "def entity_linking_nltk(some_text):\n",
    "    sentences = nltk.sent_tokenize(some_text)\n",
    "    \n",
    "    wiki_wiki = wikipediaapi.Wikipedia('en', headers={'User-Agent': 'my-custom-agent'})\n",
    "    unique_links = set()\n",
    "\n",
    "    for sent in sentences:\n",
    "        chunks = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent)))\n",
    "\n",
    "        for chunk in chunks:\n",
    "            if hasattr(chunk, 'label'):\n",
    "                entity_text = ' '.join(c[0] for c in chunk)\n",
    "                entity_page = wiki_wiki.page(entity_text)\n",
    "                \n",
    "                if entity_page.exists() and entity_page.fullurl not in unique_links:\n",
    "                    unique_links.add(entity_page.fullurl)\n",
    "                    print(f\"{entity_text}\\tWikipedia URL: {entity_page.fullurl}\")\n",
    "\n",
    "# this line calls the above function with your text as its argument\n",
    "entity_linking_nltk(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "><br>Select a short German text of your own and see how the model manages to process it.\n",
    ">><br>If you are doing the `Bonus` task, try pasting a text in some different language and report what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save your Text to a Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "«Wer bist du?» – Prinzessin Kates Antwort sagt viel über sie aus\n",
    "Bei ihrem Besuch in Schottland wurde die Prinzessin von Wales von einem Kind gefragt, wer sie sei. Die Antwort mag überraschen, doch es steckt eine Absicht dahinter. Prinzessin Kate hat bei einem öffentlichen Auftritt einmal mehr ihre Unterstützung ihrem Ehemann und Thronfolger Prinz William gegenüber bewiesen. Bei ihrem kürzlichen Besuch in Schottland begrüssten die beiden Royals wie üblich zahlreiche Fans – auch ganz kleine vor einer Primarschule. Zunächst schwärmte Kate, die wie so oft alle Blicke auf sich zog, von der schottischen «Burghead»-Grundschule und sagte: «Ich finde die Schule ganz toll!» Dann kam es zu einer niedlichen Konversation zwischen Kate und einem Schuljungen. Dieser fragte die Prinzessin ganz unverblümt: «Wer bist du?» – eine Frage, die Kate wohl nicht alle Tage gestellt bekommt. Doch die Prinzessin reagiert, wie immer in der Öffentlichkeit, gelassen und gefasst: «Ich bin mit ihm verheiratet», sagt sie an den Jungen gewandt und zeigt dabei auf Prinz William, der unweit neben ihr steht.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in nltk.sent_tokenize(text):\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "        if hasattr(chunk, 'label'):\n",
    "            print(chunk.label(), ' '.join(c[0] for c in chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEL: Linking to Wikipedia Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_linking_nltk(some_text):\n",
    "    sentences = nltk.sent_tokenize(some_text)\n",
    "    \n",
    "    # NOTE you will need to substitute 'de' with your language, if doing the bonus task. The rest remains unchanged\n",
    "    wiki_wiki = wikipediaapi.Wikipedia('de', headers={'User-Agent': 'my-custom-agent'}) \n",
    "    unique_links = set()\n",
    "\n",
    "    for sent in sentences:\n",
    "        chunks = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent)))\n",
    "\n",
    "        for chunk in chunks:\n",
    "            if hasattr(chunk, 'label'):\n",
    "                entity_text = ' '.join(c[0] for c in chunk)\n",
    "                entity_page = wiki_wiki.page(entity_text)\n",
    "                \n",
    "                if entity_page.exists() and entity_page.fullurl not in unique_links:\n",
    "                    unique_links.add(entity_page.fullurl)\n",
    "                    print(f\"{entity_text}\\tWikipedia URL: {entity_page.fullurl}\")\n",
    "\n",
    "\n",
    "entity_linking_nltk(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('mytest')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1458ad7cc56cc9c685f039d0257bb8036bcb5bd3b0a0c685c6d9f42bc54bce7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
